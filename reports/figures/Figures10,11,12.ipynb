{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9979d312-0478-4d3f-9c42-546fc4588279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Code for Figures 10, 11, and 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ffa0ce-90d8-4ea4-9522-f83ecf429f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63e003e-712b-4d89-9838-7060bda55fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pathlib as pl\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2185bcc-4c4e-401b-b494-3dd5668c07ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a23e8c7-247d-4850-af1f-8a731ecc5ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ras_data_dir = \"/dbfs/mnt/lwi-common/LWI_Production_forPurdue/LWI_Production_TCs_forPurdue/\"\n",
    "elev_data_file = \"/dbfs/mnt/lwi-transition-zone/data/pilot_reanalysis/no/prcp/ras_output/Amite_20200114.p01.tmp.hdf\"\n",
    "out_data_dir = \"/dbfs/mnt/lwi-transition-zone/data/pilot_reanalysis/prod_no_aleatory/\"\n",
    "prob_mass_dir = '/dbfs/FileStore/LWI_hydro/prob_masses/prob_masses.csv'\n",
    "amite_geo_dir =  '/Volumes/lwi/default/transition-zone-data/pilot_reanalysis/output_data'\n",
    "hdf_data_dir = pl.Path('/dbfs/mnt/lwi-transition-zone/data/pilot_reanalysis/no/prcp/ras_output') #Maybe delete if not used!\n",
    "\n",
    "antecedent_conds = [5,25,50,75,95]\n",
    "events_per_ac = 100\n",
    "recurrence_rate = 1.184297\n",
    "non_trop_recurrence_rate = 44/18.0\n",
    "ac_probs = [0.12888,0.274451,0.194052,0.294581,0.108036]\n",
    "#bias is modelled-true\n",
    "abs_bias=0.4223155\n",
    "abs_sd= 2.020001\n",
    "#relative bias is expectation of (modelled-true)/modelled\n",
    "rel_bias= -0.07017379\n",
    "#rel sd is sd of modelled-true)\n",
    "rel_sd = 0.448964\n",
    "\n",
    "prob_mass_file = pl.Path(prob_mass_dir)\n",
    "prob_mass_df = pd.read_csv(prob_mass_file)\n",
    "storm_ids = prob_mass_df.storm_id.values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41872ebb-64e9-4fa9-b4e7-817afc10f7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fdb43d8-cfdc-4852-8a6b-da4e011aeca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calc_annualized_cdf_val(in_df, recurrence_rate, cum_prob_col):\n",
    "    return np.exp(-recurrence_rate * (1 - in_df[cum_prob_col]))\n",
    "\n",
    "def get_coordinates_at_percentage(line, percentage):\n",
    "    if not 0 <= percentage <= 1:\n",
    "        raise ValueError(\"Percentage must be between 0 and 1.\")\n",
    "    \n",
    "    total_length = line.length\n",
    "    target_distance = total_length * percentage\n",
    "    point = line.interpolate(target_distance)\n",
    "    return point.x, point.y\n",
    "\n",
    "def find_closest_ras_ids(geo_frame, coordinates_list):\n",
    "    closest_ras_ids = []\n",
    "    for target_x, target_y in coordinates_list:\n",
    "        distances = np.sqrt((geo_frame['x'] - target_x) ** 2 + (geo_frame['y'] - target_y) ** 2)\n",
    "        closest_ras_ids.append(geo_frame.loc[distances.idxmin(), 'ras_id'])\n",
    "    return closest_ras_ids\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def kde_quantile(data, quantile, prob_mass=None, bounds=(0, 100), bandwidth_method='scott',\n",
    "                n_points=1000, reflection_padding=True, weight_method='resample', \n",
    "                n_resample=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    Estimate quantile from weighted KDE with tuning options\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        The data points\n",
    "    quantile : float\n",
    "        The quantile to estimate (between 0 and 1)\n",
    "    prob_mass : array-like, optional\n",
    "        Probability masses for each data point. If None, uniform weights are used.\n",
    "        Will be normalized to sum to 1.\n",
    "    bounds : tuple\n",
    "        (min, max) bounds for the data\n",
    "    bandwidth_method : str or float\n",
    "        Bandwidth method for KDE:\n",
    "        - 'scott': Scott's rule (default)\n",
    "        - 'silverman': Silverman's rule  \n",
    "        - float: Custom bandwidth factor (e.g., 0.5, 1.5)\n",
    "    n_points : int\n",
    "        Number of evaluation points for the KDE (default: 1000)\n",
    "        Higher values give smoother results but are slower\n",
    "    reflection_padding : bool\n",
    "        Whether to use reflection padding at boundaries (default: True)\n",
    "        Helps with boundary effects but adds computation\n",
    "    weight_method : str\n",
    "        Method for handling weights:\n",
    "        - 'resample': Resample data points based on weights (default, robust)\n",
    "        - 'direct': Directly weight the kernel contributions (more precise)\n",
    "        - 'hybrid': Use both approaches and average (slower but most accurate)\n",
    "    n_resample : int, optional\n",
    "        Number of resampled points for 'resample' method. \n",
    "        If None, uses max(1000, len(data) * 10)\n",
    "    random_seed : int, optional\n",
    "        Random seed for reproducible resampling\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : The estimated quantile value\n",
    "    \"\"\"\n",
    "    if len(data) == 0 or np.all(np.isnan(data)):\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # Handle probability masses\n",
    "    if prob_mass is None:\n",
    "        prob_mass = np.ones(len(data))\n",
    "    else:\n",
    "        prob_mass = np.asarray(prob_mass)\n",
    "        if len(prob_mass) != len(data):\n",
    "            raise ValueError(\"prob_mass must have same length as data\")\n",
    "    \n",
    "    # Remove NaN values and corresponding weights\n",
    "    valid_mask = ~np.isnan(data) & ~np.isnan(prob_mass) & (prob_mass > 0)\n",
    "    clean_data = data[valid_mask]\n",
    "    clean_weights = prob_mass[valid_mask]\n",
    "    \n",
    "    if len(clean_data) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    clean_weights = clean_weights / np.sum(clean_weights)\n",
    "    \n",
    "    # Ensure data is within bounds\n",
    "    clean_data = np.clip(clean_data, bounds[0], bounds[1])\n",
    "    \n",
    "    # If all values are the same, return that value\n",
    "    if np.std(clean_data) == 0:\n",
    "        return clean_data[0]\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    # Apply reflection padding if requested\n",
    "    if reflection_padding:\n",
    "        range_width = bounds[1] - bounds[0]\n",
    "        reflected_data = []\n",
    "        reflected_weights = []\n",
    "        \n",
    "        # Original data\n",
    "        reflected_data.extend(clean_data)\n",
    "        reflected_weights.extend(clean_weights)\n",
    "        \n",
    "        # Reflect across lower bound\n",
    "        lower_reflected = bounds[0] - (clean_data - bounds[0])\n",
    "        lower_mask = lower_reflected >= bounds[0] - range_width\n",
    "        reflected_data.extend(lower_reflected[lower_mask])\n",
    "        reflected_weights.extend(clean_weights[lower_mask])\n",
    "        \n",
    "        # Reflect across upper bound\n",
    "        upper_reflected = bounds[1] + (bounds[1] - clean_data)\n",
    "        upper_mask = upper_reflected <= bounds[1] + range_width\n",
    "        reflected_data.extend(upper_reflected[upper_mask])\n",
    "        reflected_weights.extend(clean_weights[upper_mask])\n",
    "        \n",
    "        clean_data = np.array(reflected_data)\n",
    "        clean_weights = np.array(reflected_weights)\n",
    "        \n",
    "        # Renormalize weights after reflection\n",
    "        clean_weights = clean_weights / np.sum(clean_weights)\n",
    "    \n",
    "    # Handle different weighting methods\n",
    "    if weight_method == 'resample':\n",
    "        # Resample based on weights\n",
    "        if n_resample is None:\n",
    "            n_resample = max(1000, len(clean_data) * 10)\n",
    "        \n",
    "        sample_indices = np.random.choice(len(clean_data), size=n_resample, \n",
    "                                        replace=True, p=clean_weights)\n",
    "        weighted_samples = clean_data[sample_indices]\n",
    "        \n",
    "        # Create KDE\n",
    "        if isinstance(bandwidth_method, str):\n",
    "            kde = gaussian_kde(weighted_samples, bw_method=bandwidth_method)\n",
    "        else:\n",
    "            kde = gaussian_kde(weighted_samples)\n",
    "            kde.set_bandwidth(kde.factor * bandwidth_method)\n",
    "        \n",
    "        # Evaluate KDE\n",
    "        x_eval = np.linspace(bounds[0], bounds[1], n_points)\n",
    "        pdf_vals = kde(x_eval)\n",
    "    \n",
    "    elif weight_method == 'direct':\n",
    "        # Direct weighted kernel approach\n",
    "        x_eval = np.linspace(bounds[0], bounds[1], n_points)\n",
    "        pdf_vals = _calculate_weighted_pdf(clean_data, clean_weights, x_eval, bandwidth_method)\n",
    "    \n",
    "    elif weight_method == 'hybrid':\n",
    "        # Combine both methods\n",
    "        x_eval = np.linspace(bounds[0], bounds[1], n_points)\n",
    "        \n",
    "        # Resample method\n",
    "        if n_resample is None:\n",
    "            n_resample = max(1000, len(clean_data) * 10)\n",
    "        sample_indices = np.random.choice(len(clean_data), size=n_resample, \n",
    "                                        replace=True, p=clean_weights)\n",
    "        weighted_samples = clean_data[sample_indices]\n",
    "        \n",
    "        if isinstance(bandwidth_method, str):\n",
    "            kde = gaussian_kde(weighted_samples, bw_method=bandwidth_method)\n",
    "        else:\n",
    "            kde = gaussian_kde(weighted_samples)\n",
    "            kde.set_bandwidth(kde.factor * bandwidth_method)\n",
    "        \n",
    "        pdf_vals_resample = kde(x_eval)\n",
    "        \n",
    "        # Direct method\n",
    "        pdf_vals_direct = _calculate_weighted_pdf(clean_data, clean_weights, x_eval, bandwidth_method)\n",
    "        \n",
    "        # Average the two approaches\n",
    "        pdf_vals = (pdf_vals_resample + pdf_vals_direct) / 2\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"weight_method must be 'resample', 'direct', or 'hybrid'\")\n",
    "    \n",
    "    # Normalize to ensure it integrates to 1 over the bounded domain\n",
    "    dx = x_eval[1] - x_eval[0]\n",
    "    pdf_vals = pdf_vals / np.trapz(pdf_vals, dx=dx)\n",
    "    \n",
    "    # Create CDF\n",
    "    cdf_vals = np.cumsum(pdf_vals) * dx\n",
    "    \n",
    "    # Find quantile by interpolation\n",
    "    if quantile <= 0:\n",
    "        return bounds[0]\n",
    "    elif quantile >= 1:\n",
    "        return bounds[1]\n",
    "    else:\n",
    "        return np.interp(quantile, cdf_vals, x_eval)\n",
    "\n",
    "\n",
    "def _calculate_weighted_pdf(data, weights, x_eval, bandwidth_method):\n",
    "    \"\"\"Helper function to calculate weighted PDF directly\"\"\"\n",
    "    # Get bandwidth from scipy's KDE\n",
    "    temp_kde = gaussian_kde(data, bw_method=bandwidth_method)\n",
    "    if isinstance(bandwidth_method, str):\n",
    "        bandwidth = temp_kde.factor * np.std(data)\n",
    "    else:\n",
    "        bandwidth = bandwidth_method * temp_kde.factor * np.std(data)\n",
    "    \n",
    "    # Calculate weighted PDF manually\n",
    "    pdf_vals = np.zeros_like(x_eval)\n",
    "    for i, xi in enumerate(data):\n",
    "        # Gaussian kernel\n",
    "        kernel_vals = np.exp(-0.5 * ((x_eval - xi) / bandwidth) ** 2)\n",
    "        kernel_vals /= (bandwidth * np.sqrt(2 * np.pi))\n",
    "        pdf_vals += weights[i] * kernel_vals\n",
    "    \n",
    "    return pdf_vals\n",
    "\n",
    "\n",
    "# Simplified version that matches your original function signature\n",
    "def kde_quantile_simple(data, quantile, prob_mass=None, bounds=(0, 100), \n",
    "                       bandwidth_method='scott', n_points=1000, reflection_padding=True):\n",
    "    \"\"\"\n",
    "    Simplified version that works exactly like the original function.\n",
    "    Just calls the full version with default weight settings.\n",
    "    \"\"\"\n",
    "    return kde_quantile(data, quantile, prob_mass=prob_mass, bounds=bounds,\n",
    "                       bandwidth_method=bandwidth_method, n_points=n_points,\n",
    "                       reflection_padding=reflection_padding, weight_method='resample')\n",
    "\n",
    "\n",
    "# Additional utility functions for weight handling\n",
    "def validate_and_normalize_weights(data, weights, method='normalize'):\n",
    "    \"\"\"\n",
    "    Utility function to handle different weight validation/normalization approaches\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        The data points\n",
    "    weights : array-like\n",
    "        The weights\n",
    "    method : str\n",
    "        - 'normalize': Normalize to sum to 1 (default)\n",
    "        - 'clip': Clip negative weights to 0, then normalize\n",
    "        - 'abs': Take absolute value, then normalize\n",
    "        - 'strict': Raise error if any weight <= 0\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    weights = np.asarray(weights)\n",
    "    \n",
    "    if len(weights) != len(data):\n",
    "        raise ValueError(\"Weights must have same length as data\")\n",
    "    \n",
    "    if method == 'strict':\n",
    "        if np.any(weights <= 0):\n",
    "            raise ValueError(\"All weights must be positive\")\n",
    "        return weights / np.sum(weights)\n",
    "    \n",
    "    elif method == 'clip':\n",
    "        weights = np.clip(weights, 0, np.inf)\n",
    "        if np.sum(weights) == 0:\n",
    "            raise ValueError(\"All weights are zero after clipping\")\n",
    "        return weights / np.sum(weights)\n",
    "    \n",
    "    elif method == 'abs':\n",
    "        weights = np.abs(weights)\n",
    "        if np.sum(weights) == 0:\n",
    "            raise ValueError(\"All weights are zero after taking absolute value\")\n",
    "        return weights / np.sum(weights)\n",
    "    \n",
    "    elif method == 'normalize':\n",
    "        if np.sum(weights) == 0:\n",
    "            raise ValueError(\"Weights sum to zero\")\n",
    "        return weights / np.sum(weights)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"method must be 'normalize', 'clip', 'abs', or 'strict'\")\n",
    "\n",
    "\n",
    "def effective_sample_size(weights):\n",
    "    \"\"\"\n",
    "    Calculate the effective sample size for weighted data\n",
    "    ESS = (sum of weights)^2 / (sum of squared weights)\n",
    "    \"\"\"\n",
    "    weights = np.asarray(weights)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    return 1.0 / np.sum(normalized_weights**2)\n",
    "\n",
    "\n",
    "def weight_statistics(weights):\n",
    "    \"\"\"\n",
    "    Get statistics about the weight distribution\n",
    "    \"\"\"\n",
    "    weights = np.asarray(weights)\n",
    "    normalized_weights = weights / np.sum(weights)\n",
    "    \n",
    "    return {\n",
    "        'ess': effective_sample_size(weights),\n",
    "        'weight_variance': np.var(normalized_weights),\n",
    "        'max_weight': np.max(normalized_weights),\n",
    "        'min_weight': np.min(normalized_weights),\n",
    "        'weight_entropy': -np.sum(normalized_weights * np.log(normalized_weights + 1e-16)),\n",
    "        'concentration_ratio': np.max(normalized_weights) / np.mean(normalized_weights)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47d52692-2088-4d9d-a34d-17cb01c8f74c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data\n",
    "1. Load the compound flooding tropical cyclone data as 'storm_data_w_id'.\n",
    "2. Load the the storm surage only data as storm_data_surge_uncert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf116c31-9b03-4704-8d37-ba428d758082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tropical cyclone depth data for ras_id (model grit points) with prob weight and storm id \n",
    "storm_data_w_id = pd.read_parquet(f'{out_data_dir}/wse_dist_tc_no_uncert_vectorized_bartlett_edits.parquet')\n",
    "storm_data_surge_uncert = pd.read_parquet(f'{out_data_dir}/wse_dist_tc_no_uncert_surge_only_vectorized_bartlett_edits.parquet')\n",
    "storm_data_w_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4554fdfb-8ae6-4e95-9396-bd39027bac99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Elevations & Amite River Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a422533d-7d60-4f24-a12d-a87ae36085d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(elev_data_file) as f:\n",
    "    elevs = np.array(list(f['Geometry']['2D Flow Areas']['AmiteMaurepas']['Cells Minimum Elevation']))\n",
    "    xy = list(f['Geometry']['2D Flow Areas']['AmiteMaurepas']['Cells Center Coordinate'])\n",
    "\n",
    "geo_frame = pd.DataFrame(xy, columns = ['x','y'])\n",
    "geo_frame['ras_id'] = range(1,len(elevs)+1)\n",
    "geo_frame['elevs'] = elevs\n",
    "\n",
    "wkt_string = 'PROJCS[\"USA_Contiguous_Albers_Equal_Area_Conic_USGS_version\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers\"],PARAMETER[\"false_easting\",0.0],PARAMETER[\"false_northing\",0.0],PARAMETER[\"central_meridian\",-96.0],PARAMETER[\"standard_parallel_1\",29.5],PARAMETER[\"standard_parallel_2\",45.5],PARAMETER[\"latitude_of_origin\",23.0],UNIT[\"Foot_US\",0.3048006096012192]]'\n",
    "amite_line = gpd.read_file(f'{amite_geo_dir}/amite_line_pass_manchac_centered_userpoints.json').to_crs(wkt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deffc173-c82f-45f8-804e-982a80f34a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Plot River Line\n",
    "Plot the river line and calculate coordinates for every 5 km along the river. Also retreive a list of coordinates for points at 5 km increments along the Amite river line, moving from upstream to downstream through Lake Maurepas to the outlet to the inlet at Lake Ponchartrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0b7f89-003f-4065-9145-ec9de524590c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "total_length_km = amite_line.geometry.iloc[0].length * 0.0003048\n",
    "percentages = [p / total_length_km for p in range(0, 116, 5)]\n",
    "coordinates_list = [get_coordinates_at_percentage(amite_line.geometry.iloc[0], p) for p in percentages]\n",
    "\n",
    "# Plot the line and the coordinates\n",
    "plt.figure(figsize=(10, 6))\n",
    "x, y = amite_line.geometry.iloc[0].xy\n",
    "plt.plot(x, y, label='Amite Line')\n",
    "\n",
    "for i, coords in enumerate(coordinates_list):\n",
    "    plt.scatter(*coords, label=f'Point at {int(percentages[i]*100)}%', zorder=5)\n",
    "\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Amite Line with Points at Specified Percentages')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "plt.text(1.05, 0.5, f'Total Length: {total_length_km:.2f} km', transform=plt.gca().transAxes, fontsize=12, verticalalignment='center')\n",
    "plt.show()\n",
    "\n",
    "coordinates_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987b8e65-7c7c-4f47-aa13-5fafff513ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "unique_ras_ids = storm_data_w_id['ras_id'].unique()\n",
    "geo_frame_filtered = geo_frame[geo_frame['ras_id'].isin(unique_ras_ids)]\n",
    "geo_frame_filtered\n",
    "\n",
    "# Example usage\n",
    "coordinates_list = coordinates_list  # Use the list of coordinates from the previous cell\n",
    "closest_ras_ids = find_closest_ras_ids(geo_frame_filtered, coordinates_list)\n",
    "closest_ras_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e490350-03d6-45f0-8ab1-69684ed2994d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Extract data for just the closest_ras_ids, those ras_ids (cells) intersected with the points of interest every 5 km along the Amite river transect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10de8724-340f-4024-a32d-ef251654357c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storm_data_w_id_filtered = storm_data_w_id[storm_data_w_id['ras_id'].isin(closest_ras_ids)]\n",
    "storm_data_w_id_filtered = storm_data_w_id_filtered.rename(columns={\"depth\": \"depth_raw\"})\n",
    "storm_data_w_id_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db3ae95b-076d-4929-9f22-12658a731a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Calculate the CDF and the return period. Also calculate the bias adjusted depth, though this is not used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a698d584-8720-4896-b362-b33e10842347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_table = storm_data_w_id_filtered.groupby(['ras_id']).apply(\n",
    "    lambda g: g.sort_values('depth_raw', ascending=True, na_position='first')\n",
    ").reset_index(drop=True)\n",
    "\n",
    "storm_data_all_uncert = grouped_table\n",
    "\n",
    "storm_data_all_uncert['cum_prob'] = storm_data_all_uncert.groupby('ras_id')['prob'].cumsum()\n",
    "\n",
    "storm_data_all_uncert['annualized_cdf_val'] = calc_annualized_cdf_val(storm_data_all_uncert, recurrence_rate, 'cum_prob')\n",
    "storm_data_all_uncert['return_period'] = 1/(1-storm_data_all_uncert['annualized_cdf_val'])\n",
    "\n",
    "storm_data_all_uncert['bias_abs'] = abs_bias\n",
    "storm_data_all_uncert['bias_rel'] = rel_bias*storm_data_all_uncert['depth_raw']\n",
    "storm_data_all_uncert['bias_bounded'] = np.minimum(np.abs(storm_data_all_uncert['bias_abs']),np.abs(storm_data_all_uncert['bias_rel']))*np.sign(storm_data_all_uncert['bias_abs'])\n",
    "storm_data_all_uncert['depth_adj'] = np.maximum(storm_data_all_uncert['depth_raw'] - storm_data_all_uncert['bias_bounded'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "636915c3-b22a-4465-9d04-884c9b9ba2b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extract data for just the ras_ids coincidental with the points of interest along the Amite River Transect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd004c8-25c9-4971-904e-44ea7a5f0376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cdf_data_list = [\n",
    "    storm_data_all_uncert[storm_data_all_uncert['ras_id'] == ras_id].copy()\n",
    "    for ras_id in closest_ras_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "485cfd3a-6a63-4a04-8771-664afdaf85ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extract events withint +/- 0.5 inches (roughly 0.042 ft) for a series of return period events of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6774c63-dfb8-4184-a130-9f658c9c1a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta = 0.042\n",
    "return_periods = [10, 50, 100, 500]\n",
    "cdf_data_frames = []\n",
    "\n",
    "# Pre-merge once per df (not inside inner loop!)\n",
    "storm_merge = storm_data_surge_uncert[['storm_id', 'ras_id', 'depth_raw']].rename(\n",
    "    columns={'depth_raw': 'depth_storm_raw'}\n",
    ")\n",
    "\n",
    "for idx, df in enumerate(cdf_data_list):\n",
    "\n",
    "    df_merged = df.merge(storm_merge, on=['storm_id', 'ras_id'], how='left', validate='m:1')\n",
    "\n",
    "    # Precompute min surge depth for this ras_id.. subtract out to make everything relative to the 1-year event, approximately.\n",
    "    min_depth_adj_row_surge = storm_data_surge_uncert.loc[\n",
    "        storm_data_surge_uncert['ras_id'] == closest_ras_ids[idx], 'depth_raw'\n",
    "    ].min()\n",
    "                         \n",
    "    df_list = []\n",
    "    for rp in return_periods:\n",
    "        # Faster closest row\n",
    "        idx_closest = (df_merged['return_period'] - rp).abs().idxmin()\n",
    "        depth_raw_value = df_merged.loc[idx_closest, 'depth_raw']\n",
    "\n",
    "        # Filter efficiently\n",
    "        mask = (df_merged['depth_raw'] >= depth_raw_value - delta) & (df_merged['depth_raw'] <= depth_raw_value + delta)\n",
    "        cdf_data_rp = df_merged.loc[mask].copy()\n",
    "\n",
    "        # Compute derived cols\n",
    "        cdf_data_rp['depth_base_raw'] = cdf_data_rp['depth_raw'] - min_depth_adj_row_surge\n",
    "        cdf_data_rp['depth_base_storm_raw'] = cdf_data_rp['depth_storm_raw'] - min_depth_adj_row_surge\n",
    "        cdf_data_rp['depth_hydrologic'] = np.maximum(cdf_data_rp['depth_raw'] - cdf_data_rp['depth_storm_raw'], 0)\n",
    "        cdf_data_rp['depth_hydrologic_percentage'] = (\n",
    "            cdf_data_rp['depth_hydrologic'] / cdf_data_rp['depth_base_raw'] * 100\n",
    "        )\n",
    "        cdf_data_rp['depth_surge_percentage'] = (\n",
    "            (cdf_data_rp['depth_base_raw']- cdf_data_rp['depth_hydrologic'])/ cdf_data_rp['depth_base_raw'] * 100\n",
    "        )\n",
    "\n",
    "        df_list.append(cdf_data_rp)\n",
    "\n",
    "    cdf_data_frames.append(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f6ed9a7-cbd7-4560-acc9-7c33230b91bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Figure 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f96dbb-5225-4fa7-be25-b05d434922f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Plot four panels (2x2 grid) ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Define CFTZ extents (km ranges) for each return period\n",
    "cftz_extents = {\n",
    "    10: [(2.853293586573366, 79.08269891412961)],\n",
    "    50: [(20.800533476964578, 80.4122950467338)],\n",
    "    100: [(20.832551236178276, 105.70525390401936)],\n",
    "    500: [(27.38825964096338, 80.08948887578502),\n",
    "          (91.4973959474711, 109.16834368793245)]\n",
    "}\n",
    "\n",
    "# KDE TUNING PARAMETERS\n",
    "FULL_KDE_CONFIG = {\n",
    "    # Original options\n",
    "    'bandwidth_method': 'silverman',   # Bandwidth selection method\n",
    "    'n_points': 1000,                  # KDE evaluation resolution\n",
    "    'reflection_padding': False,       # Boundary reflection\n",
    "    \n",
    "    # Weight handling options  \n",
    "    'weight_method': 'resample',       # How to handle weights\n",
    "    'n_resample': None,                # Resample count (None = auto)\n",
    "    'random_seed': 42,                 # Reproducibility seed\n",
    "}\n",
    "\n",
    "SMOOTH_KDE_CONFIG = {\n",
    "    'bounds': (0, 100),\n",
    "    'bandwidth_method': .75,           # Slightly wider bandwidth\n",
    "    'n_points': 3000,                  # Higher resolution\n",
    "    'reflection_padding': False,        # KEY for boundary smoothing\n",
    "    'weight_method': 'direct',         # Smoother than resampling\n",
    "    'n_resample': None,                # Not used with 'direct'\n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "# Define return periods and colors\n",
    "panel1_rps = [10, 50]\n",
    "panel2_rps = [100, 500]\n",
    "# Keep original color scheme for left/right panels\n",
    "panel1_colors = {10: 'tab:red', 50: 'tab:green'}  # 10-yr=red, 50-yr=green (was 100-yr color)\n",
    "panel2_colors = {100: 'tab:orange', 500: 'tab:blue'}  # 100-yr=orange (was 50-yr color), 500-yr=blue\n",
    "line_styles = {10: '-', 50: '-', 100: '-', 500: '-'}  # All solid lines\n",
    "\n",
    "# Calculate distances along the river\n",
    "total_length_km = amite_line.geometry.iloc[0].length * 0.0003048\n",
    "\n",
    "# --- Prepare KDE-based stats for HYDROLOGIC data ---\n",
    "kde_stats_hydro = {}\n",
    "discrete_stats_hydro = {}  # Keep discrete stats for comparison\n",
    "\n",
    "# Loop through each return period for hydrologic data\n",
    "for rp in [10, 50, 100, 500]:\n",
    "    xs_list, median_list, q25_list, q75_list, distance_list = [], [], [], [], []\n",
    "    discrete_median_list, discrete_q25_list, discrete_q75_list = [], [], []\n",
    "    \n",
    "    for i, rp_dfs in enumerate(cdf_data_frames):\n",
    "        # Find dataframe corresponding to this return period\n",
    "        df = next(df_rp for df_rp, df_return in zip(rp_dfs, [10,50,100,500]) if df_return == rp)\n",
    "        \n",
    "        # Calculate distance along river for this point\n",
    "        distance_km = (i * 5) * total_length_km / 115  # Convert point index to distance\n",
    "        \n",
    "        if not df.empty:\n",
    "            data = df['depth_hydrologic_percentage'].values\n",
    "            weights = df['prob']\n",
    "            \n",
    "            # KDE-based quantiles\n",
    "            q25 = kde_quantile(data, 0.25, weights, **SMOOTH_KDE_CONFIG)\n",
    "            median_val = kde_quantile(data, 0.50, weights, **SMOOTH_KDE_CONFIG)\n",
    "            q75 = kde_quantile(data, 0.75, weights, **SMOOTH_KDE_CONFIG)\n",
    "            \n",
    "            # Discrete quantiles for comparison\n",
    "            discrete_q25 = df['depth_hydrologic_percentage'].quantile(0.25)\n",
    "            discrete_median = df['depth_hydrologic_percentage'].median()\n",
    "            discrete_q75 = df['depth_hydrologic_percentage'].quantile(0.75)\n",
    "        else:\n",
    "            q25 = median_val = q75 = np.nan\n",
    "            discrete_q25 = discrete_median = discrete_q75 = np.nan\n",
    "            \n",
    "        xs_list.append(i)\n",
    "        distance_list.append(distance_km)\n",
    "        median_list.append(median_val)\n",
    "        q25_list.append(q25)\n",
    "        q75_list.append(q75)\n",
    "        \n",
    "        discrete_median_list.append(discrete_median)\n",
    "        discrete_q25_list.append(discrete_q25)\n",
    "        discrete_q75_list.append(discrete_q75)\n",
    "    \n",
    "    # Store KDE results for hydrologic\n",
    "    kde_stats_hydro[rp] = pd.DataFrame({\n",
    "        'i': xs_list,\n",
    "        'distance_km': distance_list,\n",
    "        'median': median_list,\n",
    "        'q25': q25_list,\n",
    "        'q75': q75_list\n",
    "    })\n",
    "    \n",
    "    # Store discrete results for comparison\n",
    "    discrete_stats_hydro[rp] = pd.DataFrame({\n",
    "        'i': xs_list,\n",
    "        'distance_km': distance_list,\n",
    "        'median': discrete_median_list,\n",
    "        'q25': discrete_q25_list,\n",
    "        'q75': discrete_q75_list\n",
    "    })\n",
    "\n",
    "# --- Prepare KDE-based stats for SURGE data ---\n",
    "kde_stats_surge = {}\n",
    "discrete_stats_surge = {}  # Keep discrete stats for comparison\n",
    "\n",
    "# Loop through each return period for surge data\n",
    "for rp in [10, 50, 100, 500]:\n",
    "    xs_list, median_list, q25_list, q75_list, distance_list = [], [], [], [], []\n",
    "    discrete_median_list, discrete_q25_list, discrete_q75_list = [], [], []\n",
    "    \n",
    "    for i, rp_dfs in enumerate(cdf_data_frames):\n",
    "        # Find dataframe corresponding to this return period\n",
    "        df = next(df_rp for df_rp, df_return in zip(rp_dfs, [10,50,100,500]) if df_return == rp)\n",
    "        \n",
    "        # Calculate distance along river for this point\n",
    "        distance_km = (i * 5) * total_length_km / 115  # Convert point index to distance\n",
    "        \n",
    "        if not df.empty:\n",
    "            data = df['depth_surge_percentage'].values  # CHANGED to surge data\n",
    "            weights = df['prob']\n",
    "            \n",
    "            # KDE-based quantiles\n",
    "            q25 = kde_quantile(data, 0.25, weights, **SMOOTH_KDE_CONFIG)\n",
    "            median_val = kde_quantile(data, 0.50, weights, **SMOOTH_KDE_CONFIG)\n",
    "            q75 = kde_quantile(data, 0.75, weights, **SMOOTH_KDE_CONFIG)\n",
    "            \n",
    "            # Discrete quantiles for comparison\n",
    "            discrete_q25 = df['depth_surge_percentage'].quantile(0.25)\n",
    "            discrete_median = df['depth_surge_percentage'].median()\n",
    "            discrete_q75 = df['depth_surge_percentage'].quantile(0.75)\n",
    "        else:\n",
    "            q25 = median_val = q75 = np.nan\n",
    "            discrete_q25 = discrete_median = discrete_q75 = np.nan\n",
    "            \n",
    "        xs_list.append(i)\n",
    "        distance_list.append(distance_km)\n",
    "        median_list.append(median_val)\n",
    "        q25_list.append(q25)\n",
    "        q75_list.append(q75)\n",
    "        \n",
    "        discrete_median_list.append(discrete_median)\n",
    "        discrete_q25_list.append(discrete_q25)\n",
    "        discrete_q75_list.append(discrete_q75)\n",
    "    \n",
    "    # Store KDE results for surge\n",
    "    kde_stats_surge[rp] = pd.DataFrame({\n",
    "        'i': xs_list,\n",
    "        'distance_km': distance_list,\n",
    "        'median': median_list,\n",
    "        'q25': q25_list,\n",
    "        'q75': q75_list\n",
    "    })\n",
    "    \n",
    "    # Store discrete results for comparison\n",
    "    discrete_stats_surge[rp] = pd.DataFrame({\n",
    "        'i': xs_list,\n",
    "        'distance_km': distance_list,\n",
    "        'median': discrete_median_list,\n",
    "        'q25': discrete_q25_list,\n",
    "        'q75': discrete_q75_list\n",
    "    })\n",
    "\n",
    "def add_cftz_bracket(ax, df_rp, rp, position=\"top\"):\n",
    "    \"\"\"Draw horizontal bracket at top or bottom with vertical connectors to curve.\"\"\"\n",
    "    y_vals = df_rp['median'].values\n",
    "    for (xmin, xmax) in cftz_extents[rp]:\n",
    "        # get curve section\n",
    "        mask = (df_rp['distance_km'] >= xmin) & (df_rp['distance_km'] <= xmax)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        x_section = df_rp['distance_km'][mask]\n",
    "        y_section = y_vals[mask]\n",
    "\n",
    "        # vertical connector positions\n",
    "        y_min, y_max = np.nanmin(y_section), np.nanmax(y_section)\n",
    "\n",
    "        if position == \"top\":\n",
    "            y_line = 100*1.05 #ax.get_ylim()[1] * 1.05  # above axis\n",
    "            va, offset = \"bottom\", .5\n",
    "        else:\n",
    "            y_line = -10 #ax.get_ylim()[0] - (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.05  # below axis\n",
    "            va, offset = \"top\", -.5\n",
    "\n",
    "        # draw horizontal bracket line\n",
    "        ax.hlines(y_line, xmin, xmax, colors=\"black\", linestyles=\"-\", alpha=0.5)\n",
    "\n",
    "        # vertical connectors\n",
    "        ax.vlines([xmin, xmax], [y_max, y_max] if position==\"top\" else [y_min, y_min],\n",
    "                  y_line, colors=\"gray\", linestyles=\"-\", alpha=0.5)\n",
    "\n",
    "        # label in middle of bracket\n",
    "        ax.text((xmin + xmax)/2, y_line + offset*(ax.get_ylim()[1]-ax.get_ylim()[0])/100,\n",
    "                f\"CFTZ ({rp}-yr)\", color=\"k\", ha=\"center\", va=va, fontsize=10)\n",
    "\n",
    "# ---------------- TOP LEFT Panel: Hydrologic 10-yr (bottom), 50-yr (top) ----------------\n",
    "ax = axes[0, 0]\n",
    "for i, rp in enumerate(panel1_rps):\n",
    "    df_rp = kde_stats_hydro[rp]\n",
    "    markers = ['s', 'o']\n",
    "    ax.plot(df_rp['distance_km'], df_rp['median'], color=panel1_colors[rp],\n",
    "            linestyle=line_styles[rp], marker=markers[i], markerfacecolor='none',\n",
    "            label=f'{rp}-yr', markersize=7)\n",
    "    ax.fill_between(df_rp['distance_km'], df_rp['q25'], df_rp['q75'],\n",
    "                    color=panel1_colors[rp], alpha=0.2)\n",
    "\n",
    "add_cftz_bracket(ax, kde_stats_hydro[10], 10, position=\"top\")\n",
    "add_cftz_bracket(ax, kde_stats_hydro[50], 50, position=\"bottom\")\n",
    "\n",
    "ax.set_ylabel(\n",
    "    'Hydrologic-attributed Depth (%):\\n Median and 25th–75th interquartile range',\n",
    "    fontsize=16\n",
    ")\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=12)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_title('Hydrologic Contribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ---------------- TOP RIGHT Panel: Hydrologic 100-yr (bottom), 500-yr (top) ----------------\n",
    "ax = axes[0, 1]\n",
    "for i, rp in enumerate(panel2_rps):\n",
    "    df_rp = kde_stats_hydro[rp]\n",
    "    markers = ['s', 'o']\n",
    "    ax.plot(df_rp['distance_km'], df_rp['median'], color=panel2_colors[rp],\n",
    "            linestyle=line_styles[rp], marker=markers[i], markerfacecolor='none',\n",
    "            label=f'{rp}-yr', markersize=7)\n",
    "    ax.fill_between(df_rp['distance_km'], df_rp['q25'], df_rp['q75'],\n",
    "                    color=panel2_colors[rp], alpha=0.2)\n",
    "\n",
    "add_cftz_bracket(ax, kde_stats_hydro[100], 100, position=\"top\")\n",
    "add_cftz_bracket(ax, kde_stats_hydro[500], 500, position=\"bottom\")\n",
    "\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=12, loc='lower left')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_title('Hydrologic Contribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ---------------- BOTTOM LEFT Panel: Surge 10-yr (bottom), 50-yr (top) ----------------\n",
    "ax = axes[1, 0]\n",
    "for i, rp in enumerate(panel1_rps):\n",
    "    df_rp = kde_stats_surge[rp]\n",
    "    markers = ['s', 'o']\n",
    "    ax.plot(df_rp['distance_km'], df_rp['median'], color=panel1_colors[rp],\n",
    "            linestyle=line_styles[rp], marker=markers[i], markerfacecolor='none',\n",
    "            label=f'{rp}-yr', markersize=7)\n",
    "    ax.fill_between(df_rp['distance_km'], df_rp['q25'], df_rp['q75'],\n",
    "                    color=panel1_colors[rp], alpha=0.2)\n",
    "\n",
    "add_cftz_bracket(ax, kde_stats_surge[10], 10, position=\"top\")\n",
    "add_cftz_bracket(ax, kde_stats_surge[50], 50, position=\"bottom\")\n",
    "\n",
    "ax.set_xlabel('Distance Along River (km)', fontsize=16)\n",
    "ax.set_ylabel(\n",
    "    'Surge-attributed Depth (%):\\n Median and 25th–75th interquartile range',\n",
    "    fontsize=16\n",
    ")\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=12, loc='lower right')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_title('Surge Contribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ---------------- BOTTOM RIGHT Panel: Surge 100-yr (bottom), 500-yr (top) ----------------\n",
    "ax = axes[1, 1]\n",
    "for i, rp in enumerate(panel2_rps):\n",
    "    df_rp = kde_stats_surge[rp]\n",
    "    markers = ['s', 'o']\n",
    "    ax.plot(df_rp['distance_km'], df_rp['median'], color=panel2_colors[rp],\n",
    "            linestyle=line_styles[rp], marker=markers[i], markerfacecolor='none',\n",
    "            label=f'{rp}-yr', markersize=7)\n",
    "    ax.fill_between(df_rp['distance_km'], df_rp['q25'], df_rp['q75'],\n",
    "                    color=panel2_colors[rp], alpha=0.2)\n",
    "\n",
    "add_cftz_bracket(ax, kde_stats_surge[100], 100, position=\"top\")\n",
    "add_cftz_bracket(ax, kde_stats_surge[500], 500, position=\"bottom\")\n",
    "\n",
    "ax.set_xlabel('Distance Along River (km)', fontsize=16)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=12, loc='lower left')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_title('Surge Contribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ---------------- Shared formatting ----------------\n",
    "# Share y-axes within rows\n",
    "axes[0, 0].sharey(axes[0, 1])  # Top row shares y-axis\n",
    "axes[1, 0].sharey(axes[1, 1])  # Bottom row shares y-axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hydrologic_and_surge_contribution_with_CFTZ_brackets.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540f243-ed72-4183-9d00-4c1e68b61d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Other Figure 10 Variations (NOT USED IN PAPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbcd957a-cbd0-4f0d-941a-499dc66c414a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Plot two panels ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Define CFTZ extents (km ranges) for each return period\n",
    "cftz_extents = {\n",
    "    10: [(2.853293586573366, 79.08269891412961)],\n",
    "    50: [(20.800533476964578, 80.4122950467338)],\n",
    "    100: [(20.832551236178276, 105.70525390401936)],\n",
    "    500: [(27.38825964096338, 80.08948887578502),\n",
    "          (91.4973959474711, 109.16834368793245)]\n",
    "}\n",
    "\n",
    "\n",
    "# KDE TUNING PARAMETERS\n",
    "#KDE_CONFIG = {\n",
    "#    'bandwidth_method': 'silverman',  # 'scott', 'silverman', or float (e.g., 0.5, 1.5)\n",
    "#    'n_points': 1000,            # Resolution of KDE evaluation\n",
    "#    'reflection_padding': False,   # Use reflection at boundaries\n",
    "#}\n",
    "\n",
    "FULL_KDE_CONFIG = {\n",
    "    # Original options\n",
    "    'bandwidth_method': 'silverman',   # Bandwidth selection method\n",
    "    'n_points': 1000,                  # KDE evaluation resolution\n",
    "    'reflection_padding': False,       # Boundary reflection\n",
    "    \n",
    "    # Weight handling options  \n",
    "    'weight_method': 'resample',       # How to handle weights\n",
    "    'n_resample': None,                # Resample count (None = auto)\n",
    "    'random_seed': 42,                 # Reproducibility seed\n",
    "}\n",
    "\n",
    "SMOOTH_KDE_CONFIG = {\n",
    "    'bounds': (0, 100),\n",
    "    'bandwidth_method': .75,           # Slightly wider bandwidth\n",
    "    'n_points': 3000,                  # Higher resolution\n",
    "    'reflection_padding': False,        # KEY for boundary smoothing\n",
    "    'weight_method': 'direct',         # Smoother than resampling\n",
    "    'n_resample': None,                # Not used with 'direct'\n",
    "    'random_seed': 42,\n",
    "}\n",
    "\n",
    "# Define return periods and colors\n",
    "panel1_rps = [10, 50]\n",
    "panel2_rps = [100, 500]\n",
    "# Keep original color scheme for left/right panels\n",
    "panel1_colors = {10: 'tab:red', 50: 'tab:green'}  # 10-yr=red, 50-yr=green (was 100-yr color)\n",
    "panel2_colors = {100: 'tab:orange', 500: 'tab:blue'}  # 100-yr=orange (was 50-yr color), 500-yr=blue\n",
    "line_styles = {10: '-', 50: '-', 100: '-', 500: '-'}  # All solid lines\n",
    "\n",
    "\n",
    "# Calculate distances along the river\n",
    "total_length_km = amite_line.geometry.iloc[0].length * 0.0003048\n",
    "\n",
    "# --- Prepare KDE-based stats ---\n",
    "kde_stats = {}\n",
    "discrete_stats = {}  # Keep discrete stats for comparison\n",
    "\n",
    "# Loop through each return period\n",
    "for rp in [10, 50, 100, 500]:\n",
    "    xs_list, median_list, q25_list, q75_list, distance_list = [], [], [], [], []\n",
    "    discrete_median_list, discrete_q25_list, discrete_q75_list = [], [], []\n",
    "    \n",
    "    for i, rp_dfs in enumerate(cdf_data_frames):\n",
    "        # Find dataframe corresponding to this return period\n",
    "        df = next(df_rp for df_rp, df_return in zip(rp_dfs, [10,50,100,500]) if df_return == rp)\n",
    "        \n",
    "        # Calculate distance along river for this point\n",
    "        distance_km = (i * 5) * total_length_km / 115  # Convert point index to distance\n",
    "        \n",
    "        if not df.empty:\n",
    "            data = df['depth_hydrologic_percentage'].values\n",
    "\n",
    "            weights = df['prob']\n",
    "            \n",
    "            # KDE-based quantiles\n",
    "            q25 = kde_quantile(data, 0.25,weights, **SMOOTH_KDE_CONFIG)\n",
    "            median_val = kde_quantile(data, 0.50,weights, **SMOOTH_KDE_CONFIG)\n",
    "            q75 = kde_quantile(data, 0.75,weights, **SMOOTH_KDE_CONFIG)\n",
    "            \n",
    "            # Discrete quantiles for comparison\n",
    "            discrete_q25 = df['depth_hydrologic_percentage'].quantile(0.25)\n",
    "            discrete_median = df['depth_hydrologic_percentage'].median()\n",
    "            discrete_q75 = df['depth_hydrologic_percentage'].quantile(0.75)\n",
    "        else:\n",
    "            q25 = median_val = q75 = np.nan\n",
    "            discrete_q25 = discrete_median = discrete_q75 = np.nan\n",
    "            \n",
    "        xs_list.append(i)\n",
    "        distance_list.append(distance_km)\n",
    "        median_list.append(median_val)\n",
    "        q25_list.append(q25)\n",
    "        q75_list.append(q75)\n",
    "        \n",
    "        discrete_median_list.append(discrete_median)\n",
    "        discrete_q25_list.append(discrete_q25)\n",
    "        discrete_q75_list.append(discrete_q75)\n",
    "    \n",
    "    # Store KDE results\n",
    "    kde_stats[rp] = pd.DataFrame({\n",
    "        'i': xs_list,\n",
    "        'distance_km': distance_list,\n",
    "        'median': median_list,\n",
    "        'q25': q25_list,\n",
    "        'q75': q75_list\n",
    "    })\n",
    "    \n",
    "    # Store discrete results for comparison\n",
    "    discrete_stats[rp] = pd.DataFrame({\n",
    "        'i': xs_list,\n",
    "        'distance_km': distance_list,\n",
    "        'median': discrete_median_list,\n",
    "        'q25': discrete_q25_list,\n",
    "        'q75': discrete_q75_list\n",
    "    })\n",
    "\n",
    "def add_cftz_bracket(ax, df_rp, rp, position=\"top\"):\n",
    "    \"\"\"Draw horizontal bracket at top or bottom with vertical connectors to curve.\"\"\"\n",
    "    y_vals = df_rp['median'].values\n",
    "    for (xmin, xmax) in cftz_extents[rp]:\n",
    "        # get curve section\n",
    "        mask = (df_rp['distance_km'] >= xmin) & (df_rp['distance_km'] <= xmax)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        x_section = df_rp['distance_km'][mask]\n",
    "        y_section = y_vals[mask]\n",
    "\n",
    "        # vertical connector positions\n",
    "        y_min, y_max = np.nanmin(y_section), np.nanmax(y_section)\n",
    "\n",
    "        if position == \"top\":\n",
    "            y_line = 100*1.05 #ax.get_ylim()[1] * 1.05  # above axis\n",
    "            va, offset = \"bottom\", .5\n",
    "        else:\n",
    "            y_line = -10 #ax.get_ylim()[0] - (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.05  # below axis\n",
    "            va, offset = \"top\", -.5\n",
    "\n",
    "        # draw horizontal bracket line\n",
    "        ax.hlines(y_line, xmin, xmax, colors=\"black\", linestyles=\"-\", alpha=0.5)\n",
    "\n",
    "        # vertical connectors\n",
    "        ax.vlines([xmin, xmax], [y_max, y_max] if position==\"top\" else [y_min, y_min],\n",
    "                  y_line, colors=\"gray\", linestyles=\"-\", alpha=0.5)\n",
    "\n",
    "        # label in middle of bracket\n",
    "        ax.text((xmin + xmax)/2, y_line + offset*(ax.get_ylim()[1]-ax.get_ylim()[0])/100,\n",
    "                f\"CFTZ ({rp}-yr)\", color=\"k\", ha=\"center\", va=va, fontsize=10)\n",
    "\n",
    "# ---------------- Panel 1: 10-yr (bottom), 50-yr (top) ----------------\n",
    "ax = axes[0]\n",
    "for i, rp in enumerate(panel1_rps):\n",
    "    df_rp = kde_stats[rp]\n",
    "    markers = ['s', 'o']\n",
    "    ax.plot(df_rp['distance_km'], df_rp['median'], color=panel1_colors[rp],\n",
    "            linestyle=line_styles[rp], marker=markers[i], markerfacecolor='none',\n",
    "            label=f'{rp}-yr', markersize=7)\n",
    "    ax.fill_between(df_rp['distance_km'], df_rp['q25'], df_rp['q75'],\n",
    "                    color=panel1_colors[rp], alpha=0.2)\n",
    "\n",
    "add_cftz_bracket(ax, kde_stats[10], 10, position=\"top\")\n",
    "add_cftz_bracket(ax, kde_stats[50], 50, position=\"bottom\")\n",
    "\n",
    "ax.set_xlabel('Distance Along River (km)', fontsize=16)\n",
    "ax.set_ylabel(\n",
    "    'Hydrologic-attributed Depth (%):\\n Median and 25th–75th interquartile range',\n",
    "    fontsize=16\n",
    ")\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=12)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "# ---------------- Panel 2: 100-yr (bottom), 500-yr (top) ----------------\n",
    "ax = axes[1]\n",
    "for i, rp in enumerate(panel2_rps):\n",
    "    df_rp = kde_stats[rp]\n",
    "    markers = ['s', 'o']\n",
    "    ax.plot(df_rp['distance_km'], df_rp['median'], color=panel2_colors[rp],\n",
    "            linestyle=line_styles[rp], marker=markers[i], markerfacecolor='none',\n",
    "            label=f'{rp}-yr', markersize=7)\n",
    "    ax.fill_between(df_rp['distance_km'], df_rp['q25'], df_rp['q75'],\n",
    "                    color=panel2_colors[rp], alpha=0.2)\n",
    "\n",
    "add_cftz_bracket(ax, kde_stats[100], 100, position=\"top\")\n",
    "add_cftz_bracket(ax, kde_stats[500], 500, position=\"bottom\")\n",
    "\n",
    "ax.set_xlabel('Distance Along River (km)', fontsize=16)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=12, loc='lower left')\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "# ---------------- Shared formatting ----------------\n",
    "axes[0].sharey(axes[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hydrologic_contribution_with_CFTZ_brackets.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171614bd-90db-496e-9e87-22021002c1c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# return periods and colors (consistent with earlier plots)\n",
    "return_periods = [10, 50, 100, 500]\n",
    "colors = ['tab:red', 'tab:orange', 'tab:green', 'tab:blue']\n",
    "\n",
    "# storage for results\n",
    "medians_by_rp = {rp: [] for rp in return_periods}\n",
    "\n",
    "# loop through each i (point index)\n",
    "for i, rp_dfs in enumerate(cdf_data_frames):\n",
    "    for rp, df in zip(return_periods, rp_dfs):\n",
    "        if not df.empty:\n",
    "            df=df.sort_values(by='depth_hydrologic_percentage').reset_index(drop=True)\n",
    "            df['normalized_prob'] = df['prob'].cumsum() / df['prob'].sum()\n",
    "\n",
    "            median_val = np.interp(0.5, df['normalized_prob'], df['depth_hydrologic_percentage'])\n",
    "            medians_by_rp[rp].append((i, median_val))\n",
    "        else:\n",
    "            medians_by_rp[rp].append((i, np.nan))\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10,6))\n",
    "for rp, color in zip(return_periods, colors):\n",
    "    xs = [pt[0] for pt in medians_by_rp[rp]]\n",
    "    ys = [pt[1] for pt in medians_by_rp[rp]]\n",
    "    plt.plot(xs, ys, marker='o', color=color, label=f'{rp}-yr')\n",
    "\n",
    "plt.xlabel('Point Index (i)', fontsize=12)\n",
    "plt.ylabel('Median of Hydrologic-attributed Depth (%)', fontsize=12)\n",
    "plt.title('Medians of Hydrologic-attributed Depth % Across Transect Points', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(frameon=False, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7726c5f-b4d1-4d20-a7c9-a266208f8a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# return periods and colors (consistent with earlier plots)\n",
    "return_periods = [10, 50, 100, 500]\n",
    "colors = ['tab:red', 'tab:orange', 'tab:green', 'tab:blue']\n",
    "\n",
    "# storage for results\n",
    "medians_by_rp = {rp: [] for rp in return_periods}\n",
    "\n",
    "# loop through each i (point index)\n",
    "for i, rp_dfs in enumerate(cdf_data_frames):\n",
    "    for rp, df in zip(return_periods, rp_dfs):\n",
    "        if not df.empty:\n",
    "            df=df.sort_values(by='depth_surge_percentage').reset_index(drop=True)\n",
    "            df['normalized_prob'] = df['prob'].cumsum() / df['prob'].sum()\n",
    "\n",
    "            median_val = np.interp(0.5, df['normalized_prob'], df['depth_surge_percentage'])\n",
    "            medians_by_rp[rp].append((i, median_val))\n",
    "        else:\n",
    "            medians_by_rp[rp].append((i, np.nan))\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10,6))\n",
    "for rp, color in zip(return_periods, colors):\n",
    "    xs = [pt[0] for pt in medians_by_rp[rp]]\n",
    "    ys = [pt[1] for pt in medians_by_rp[rp]]\n",
    "    plt.plot(xs, ys, marker='o', color=color, label=f'{rp}-yr')\n",
    "\n",
    "plt.xlabel('Point Index (i)', fontsize=12)\n",
    "plt.ylabel('Median of Hydrologic-attributed Depth (%)', fontsize=12)\n",
    "plt.title('Medians of Hydrologic-attributed Depth % Across Transect Points', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(frameon=False, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca5aac0-21b9-4f60-afe0-857eb0225c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# return periods and colors (consistent with earlier plots)\n",
    "return_periods = [500, 100, 50, 10]\n",
    "colors = ['tab:blue', 'tab:green', 'tab:orange', 'tab:red']\n",
    "\n",
    "# storage for results\n",
    "medians_by_rp = {rp: [] for rp in return_periods}\n",
    "\n",
    "# loop through each i (point index)\n",
    "for i, rp_dfs in enumerate(cdf_data_frames):\n",
    "    for rp, df in zip(return_periods, rp_dfs):\n",
    "        if not df.empty:\n",
    "            median_val = df['depth_hydrologic_percentage'].median()\n",
    "            medians_by_rp[rp].append((i, median_val))\n",
    "        else:\n",
    "            medians_by_rp[rp].append((i, np.nan))\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10,6))\n",
    "for rp, color in zip(return_periods, colors):\n",
    "    xs = [pt[0] for pt in medians_by_rp[rp]]\n",
    "    ys = [pt[1] for pt in medians_by_rp[rp]]\n",
    "    plt.plot(xs, ys, marker='o', color=color, label=f'{rp}-yr')\n",
    "\n",
    "plt.xlabel('Point Index (i)', fontsize=12)\n",
    "plt.ylabel('Median of Hydrologic-attributed Depth (%)', fontsize=12)\n",
    "plt.title('Medians of Hydrologic-attributed Depth % Across Transect Points', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(frameon=False, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141306b1-36e4-4137-8776-eed52e1ea190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def kde_quantile(data, quantile, bounds=(0, 100)):\n",
    "    \"\"\"\n",
    "    Estimate quantile from KDE with support on bounded interval\n",
    "    \"\"\"\n",
    "    if len(data) == 0 or np.all(np.isnan(data)):\n",
    "        return np.nan\n",
    "    \n",
    "    # Remove NaN values\n",
    "    clean_data = data[~np.isnan(data)]\n",
    "    if len(clean_data) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Ensure data is within bounds\n",
    "    clean_data = np.clip(clean_data, bounds[0], bounds[1])\n",
    "    \n",
    "    # If all values are the same, return that value\n",
    "    if np.std(clean_data) == 0:\n",
    "        return clean_data[0]\n",
    "    \n",
    "    # Create KDE\n",
    "    kde = gaussian_kde(clean_data)\n",
    "    \n",
    "    # Create evaluation points within bounds\n",
    "    x_eval = np.linspace(bounds[0], bounds[1], 1000)\n",
    "    pdf_vals = kde(x_eval)\n",
    "    \n",
    "    # Normalize to ensure it integrates to 1 over the bounded domain\n",
    "    pdf_vals = pdf_vals / np.trapz(pdf_vals, x_eval)\n",
    "    \n",
    "    # Create CDF\n",
    "    cdf_vals = np.cumsum(pdf_vals) * (x_eval[1] - x_eval[0])\n",
    "    \n",
    "    # Find quantile by interpolation\n",
    "    if quantile <= 0:\n",
    "        return bounds[0]\n",
    "    elif quantile >= 1:\n",
    "        return bounds[1]\n",
    "    else:\n",
    "        return np.interp(quantile, cdf_vals, x_eval)\n",
    "\n",
    "# return periods and colors (consistent with earlier plots)\n",
    "return_periods = [500, 100, 50, 10]\n",
    "colors = ['tab:blue', 'tab:green', 'tab:orange', 'tab:red']\n",
    "\n",
    "# storage for results\n",
    "stats_by_rp = {rp: [] for rp in return_periods}\n",
    "\n",
    "# loop through each i (point index)\n",
    "for i, rp_dfs in enumerate(cdf_data_frames):\n",
    "    for rp, df in zip(return_periods, rp_dfs):\n",
    "        if not df.empty:\n",
    "            data = df['depth_hydrologic_percentage'].values\n",
    "            \n",
    "            # Use KDE-based quantiles\n",
    "            q25 = kde_quantile(data, 0.25)\n",
    "            median_val = kde_quantile(data, 0.50)\n",
    "            q75 = kde_quantile(data, 0.75)\n",
    "            \n",
    "            stats_by_rp[rp].append((i, q25, median_val, q75))\n",
    "        else:\n",
    "            stats_by_rp[rp].append((i, np.nan, np.nan, np.nan))\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10,6))\n",
    "for rp, color in zip(return_periods, colors):\n",
    "    xs = [pt[0] for pt in stats_by_rp[rp]]\n",
    "    medians = [pt[2] for pt in stats_by_rp[rp]]\n",
    "    q25s = [pt[1] for pt in stats_by_rp[rp]]\n",
    "    q75s = [pt[3] for pt in stats_by_rp[rp]]\n",
    "\n",
    "    # line for medians\n",
    "    plt.plot(xs, medians, marker='o', color=color, label=f'{rp}-yr')\n",
    "\n",
    "    # shaded interquartile range\n",
    "    plt.fill_between(xs, q25s, q75s, color=color, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Point Index (i)', fontsize=12)\n",
    "plt.ylabel('Median of Hydrologic-attributed Depth (%)', fontsize=12)\n",
    "plt.title('KDE-Smoothed Medians with Interquartile Range of Hydrologic-attributed Depth %', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(frameon=False, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f14394c5-306a-4ba2-83f9-77a437015b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Figure 11\n",
    "Examine the distribution of the hydrologic and storm surge attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca03adb5-fd5f-4bb9-abd4-bc768867f2d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Helper for regression fitting ---\n",
    "def fit_and_predict(x, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1,1), y)\n",
    "    order = np.argsort(x)\n",
    "    return x[order], model.predict(x.reshape(-1,1))[order], model\n",
    "\n",
    "# --- Simple weighted line function ---\n",
    "def plot_weighted_line(ax, x, y, weights, color, label):\n",
    "    \"\"\"\n",
    "    Plot regression line with constant thickness and varying opacity based on local weight density\n",
    "    \"\"\"\n",
    "    # Fit regression\n",
    "    x_sorted, y_pred, model = fit_and_predict(x, y)\n",
    "    \n",
    "    # Add 5% buffer to line extent\n",
    "    x_range = x_sorted.max() - x_sorted.min()\n",
    "    x_buffer = x_range * 0\n",
    "    x_start = x_sorted.min() - x_buffer\n",
    "    x_end = x_sorted.max() + x_buffer\n",
    "    \n",
    "    # Create many points along the line for smooth shading\n",
    "    n_points = 100\n",
    "    x_line = np.linspace(x_start, x_end, n_points)\n",
    "    y_line = model.predict(x_line.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate local density along the line\n",
    "    densities = []\n",
    "    for x_val in x_line:\n",
    "        # Find points within a window around this x value\n",
    "        window_size = x_range * 0.1  # 10% of data range\n",
    "        distances = np.abs(x - x_val)\n",
    "        nearby_mask = distances <= window_size\n",
    "        \n",
    "        if np.sum(nearby_mask) > 0:\n",
    "            # Sum of weights of nearby points (higher = more density)\n",
    "            local_density = np.sum(weights[nearby_mask])\n",
    "        else:\n",
    "            # If no points nearby, use minimum density\n",
    "            local_density = 0\n",
    "        \n",
    "        densities.append(local_density)\n",
    "    \n",
    "    densities = np.array(densities)\n",
    "    \n",
    "    # Normalize densities to [0, 1] for alpha mapping\n",
    "    if densities.max() > densities.min():\n",
    "        densities_norm = (densities - densities.min()) / (densities.max() - densities.min())\n",
    "    else:\n",
    "        densities_norm = np.ones_like(densities) * 0.5\n",
    "    \n",
    "    print(f\"{label}: density range {densities_norm.min():.3f} to {densities_norm.max():.3f}\")\n",
    "    \n",
    "    # Plot line segments with varying alpha\n",
    "    for i in range(len(x_line) - 1):\n",
    "        # Alpha based on average density of this segment\n",
    "        avg_density = (densities_norm[i] + densities_norm[i + 1]) / 2\n",
    "        alpha = 0.2 + 0.8 * avg_density  # Range from 0.2 to 1.0\n",
    "        \n",
    "        # Plot this segment\n",
    "        ax.plot([x_line[i], x_line[i + 1]], [y_line[i], y_line[i + 1]], \n",
    "                color=color, linewidth=8, alpha=alpha, \n",
    "                solid_capstyle='round', zorder=5)\n",
    "    \n",
    "    # Plot thin black centerline over the entire line\n",
    "    ax.plot(x_line, y_line, color='black', linewidth=0.5, alpha=1.0, \n",
    "            solid_capstyle='round', zorder=10)\n",
    "    \n",
    "    # Legend\n",
    "    ax.plot([], [], color=color, linewidth=12, alpha=0.6, label=label)\n",
    "    \n",
    "    return x_sorted, y_pred, model\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Define total length in km\n",
    "total_length_km = amite_line.geometry.iloc[0].length * 0.0003048\n",
    "\n",
    "# Loop through the four panels\n",
    "for idx, ax in zip([4, 9, 12, 20], axes.flatten()):\n",
    "    distance_km = int((idx * 5) * total_length_km / 115)\n",
    "\n",
    "    #ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # Define datasets with labels + colors\n",
    "    df_10, df_50, df_100, df_500 = cdf_data_frames[idx]\n",
    "    \n",
    "    # Build datasets list\n",
    "    datasets = [\n",
    "        (df_500, '500-yr', 'tab:blue'),\n",
    "        (df_100, '100-yr', 'tab:green'),\n",
    "        (df_50, '50-yr', 'tab:orange'),\n",
    "        (df_10, '10-yr', 'tab:red'),\n",
    "    ]\n",
    "    \n",
    "    lines_info = []\n",
    "    \n",
    "    for df, label, color in datasets:\n",
    "        x = df['depth_hydrologic'].values*0.3048 #ft to meters\n",
    "        y = df['depth_base_storm_raw'].values*0.3048 #ft to meters\n",
    "        \n",
    "        # Normalize weights to create a proper PDF (sum to 1)\n",
    "        raw_weights = df['prob'].values\n",
    "        weights = raw_weights / raw_weights.sum()  # This is now a proper PDF\n",
    "        \n",
    "        print(f\"{label}: PDF sum = {weights.sum():.6f}\")  # Should be 1.0\n",
    "        \n",
    "        # Plot weighted line\n",
    "        x_sorted, y_pred, model = plot_weighted_line(ax, x, y, weights, color, label)\n",
    "        lines_info.append((x_sorted, y_pred, model, label, color, df))\n",
    "    \n",
    "    # Set axis limits with proper buffering\n",
    "    all_x = np.concatenate([df['depth_hydrologic'].values for df, _, _ in datasets])\n",
    "    all_y = np.concatenate([df['depth_base_storm_raw'].values for df, _, _ in datasets])\n",
    "    \n",
    "    x_range = (all_x.max() - all_x.min())\n",
    "    y_range = (all_y.max() - all_y.min())\n",
    "\n",
    "    ax.set_xlim(0 - 0.95*0.3048 , (all_x.min()+17)*0.3048)\n",
    "    ax.set_ylim(0 - 0.95*0.3048, (all_y.min()+10)*0.3048)\n",
    "\n",
    "    #ax.set_xlim(all_x.min() - 0.1 * x_range, all_x.max() + 0.1 * x_range)\n",
    "    #ax.set_ylim(all_y.min() - 0.1 * y_range, all_y.max() + 0.1 * y_range)\n",
    "    #ax.set_ylim(-.25, 12)\n",
    "    \n",
    "    # Set up the plot\n",
    "    ax.set_xlabel('Hydrologic-attributed Depth, m', fontsize=17)\n",
    "    ax.set_ylabel('Surge-attributed Depth, m', fontsize=17)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.set_title(f'Design events at {distance_km:} km along the Amite River', fontsize=18)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(frameon=False, fontsize=12, loc='upper right')\n",
    "    \n",
    "    # Add rotated text annotations\n",
    "    for x_sorted, y_pred, model, label, color, df in lines_info:\n",
    "        x_mid = x_sorted[0] #(x_sorted[0] + x_sorted[-1]) / 2\n",
    "        y_mid = np.interp(x_mid, x_sorted, y_pred)\n",
    "        \n",
    "        data_slope = (y_pred[-1] - y_pred[0]) / (x_sorted[-1] - x_sorted[0])\n",
    "        \n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "        bbox = ax.get_window_extent()\n",
    "        width_display = bbox.width\n",
    "        height_display = bbox.height\n",
    "        \n",
    "        x_scale = width_display / (xlim[1] - xlim[0])\n",
    "        y_scale = height_display / (ylim[1] - ylim[0])\n",
    "        \n",
    "        visual_slope = data_slope * (y_scale / x_scale)\n",
    "        angle_deg = np.degrees(np.arctan(visual_slope))+2.5\n",
    "        \n",
    "        avg_depth = (np.mean(df['depth_hydrologic'].values) + np.mean(df['depth_base_storm_raw'].values))*0.3048\n",
    "        offset = 0.3*0.3048\n",
    "        \n",
    "        ax.text(\n",
    "            x_mid,\n",
    "            y_mid + offset,\n",
    "            f'Compound Flood Depth {avg_depth:.2f} m',\n",
    "            color='black',\n",
    "            fontsize=12,\n",
    "            rotation=angle_deg,\n",
    "            ha='left',\n",
    "            va='top',\n",
    "            bbox=dict(facecolor='white', alpha=0.0, edgecolor='none'),\n",
    "            zorder=11\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"design_storms_weighted.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "643c2e0b-61e6-432f-9960-a0f9d463260e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Other Variations of Figure 11 (NOT USED IN PAPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a01f978-c7e7-4008-97e0-55f53ce7ab5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Helper for regression fitting ---\n",
    "def fit_and_predict(x, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1,1), y)\n",
    "    order = np.argsort(x)\n",
    "    return x[order], model.predict(x.reshape(-1,1))[order], model\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "# Define total length in km\n",
    "total_length_km = amite_line.geometry.iloc[0].length * 0.0003048\n",
    "# Loop through the four panels\n",
    "for idx, ax in zip([4, 9, 12, 20], axes.flatten()):\n",
    "    distance_km = int((idx * 5) * total_length_km / 115) # Convert point index to distance and round to nearest integer\n",
    "    # Define datasets with labels + colors\n",
    "    df_10, df_50, df_100, df_500 = cdf_data_frames[idx]\n",
    "    # build datasets list\n",
    "    datasets = [\n",
    "        (df_500, '500-yr', 'tab:blue'),\n",
    "        (df_100, '100-yr', 'tab:green'),\n",
    "        (df_50, '50-yr', 'tab:orange'),\n",
    "        (df_10, '10-yr', 'tab:red'),\n",
    "    ]\n",
    "    lines_info = []\n",
    "    for df, label, color in datasets:\n",
    "        x = df['depth_hydrologic'].values\n",
    "        y = df['depth_base_storm_raw'].values\n",
    "        # Scatter\n",
    "        ax.scatter(x, y, color=color, alpha=0.4, s=20)\n",
    "        # Regression\n",
    "        x_sorted, y_pred, model = fit_and_predict(x, y)\n",
    "        ax.plot(x_sorted, y_pred, color=color, linewidth=2, linestyle='--', label=label)\n",
    "        lines_info.append((x_sorted, y_pred, model, label, color, df))\n",
    "    # Set up the plot completely first\n",
    "    ax.set_xlabel('Hydrologic-attributed Depth (ft)', fontsize=16)\n",
    "    ax.set_ylabel('Surge-attributed Depth (ft)', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.set_title(f'Design events at {distance_km:} km along the Amite River', fontsize=17)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(frameon=False, fontsize=12, loc='upper right')\n",
    "    # Now that the plot is fully set up, calculate angles\n",
    "    for x_sorted, y_pred, model, label, color, df in lines_info:\n",
    "        # midpoint in data coordinates\n",
    "        x_mid = (x_sorted[0] + x_sorted[-1]) / 2\n",
    "        y_mid = np.interp(x_mid, x_sorted, y_pred)\n",
    "        # Get the actual slope of the line in data coordinates\n",
    "        data_slope = (y_pred[-1] - y_pred[0]) / (x_sorted[-1] - x_sorted[0])\n",
    "        # Convert slope to visual angle accounting for axis scaling\n",
    "        # Get the data-to-display transform\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "        # Get figure size in display coordinates\n",
    "        bbox = ax.get_window_extent()\n",
    "        width_display = bbox.width\n",
    "        height_display = bbox.height\n",
    "        # Calculate scaling factors\n",
    "        x_scale = width_display / (xlim[1] - xlim[0])\n",
    "        y_scale = height_display / (ylim[1] - ylim[0])\n",
    "        # Adjust slope for display coordinates\n",
    "        visual_slope = data_slope * (y_scale / x_scale)\n",
    "        angle_deg = np.degrees(np.arctan(visual_slope))\n",
    "        avg_depth = np.mean(df['depth_hydrologic'].values) + np.mean(df['depth_base_storm_raw'].values)\n",
    "        offset = 0.3  # Smaller vertical offset in data units - closer to line\n",
    "        ax.text(\n",
    "            x_mid,\n",
    "            y_mid + offset,\n",
    "            f'Compound Flood Depth {avg_depth:.2f} ft',\n",
    "            color='black',\n",
    "            fontsize=12,\n",
    "            rotation=angle_deg,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n",
    "        )\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"design_storms.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608664aa-74e2-43c2-8009-f9102fb12941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Helper for regression fitting ---\n",
    "def fit_and_predict(x, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1,1), y)\n",
    "    order = np.argsort(x)\n",
    "    return x[order], model.predict(x.reshape(-1,1))[order], model\n",
    "\n",
    "# --- Simple weighted line function ---\n",
    "def plot_weighted_line(ax, x, y, weights, color, label):\n",
    "    \"\"\"\n",
    "    Plot regression line with constant thickness and varying opacity based on local weight density\n",
    "    \"\"\"\n",
    "    # Fit regression\n",
    "    x_sorted, y_pred, model = fit_and_predict(x, y)\n",
    "    \n",
    "    # Add 5% buffer to line extent\n",
    "    x_range = x_sorted.max() - x_sorted.min()\n",
    "    x_buffer = x_range * 0.00\n",
    "    x_start = x_sorted.min() - x_buffer\n",
    "    x_end = x_sorted.max() + x_buffer\n",
    "    \n",
    "    # Create many points along the line for smooth shading\n",
    "    n_points = 100\n",
    "    x_line = np.linspace(x_start, x_end, n_points)\n",
    "    y_line = model.predict(x_line.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate local density along the line\n",
    "    densities = []\n",
    "    for x_val in x_line:\n",
    "        # Find points within a window around this x value\n",
    "        window_size = x_range * 0.1  # 10% of data range\n",
    "        distances = np.abs(x - x_val)\n",
    "        nearby_mask = distances <= window_size\n",
    "        \n",
    "        if np.sum(nearby_mask) > 0:\n",
    "            # Sum of weights of nearby points (higher = more density)\n",
    "            local_density = np.sum(weights[nearby_mask])\n",
    "        else:\n",
    "            # If no points nearby, use minimum density\n",
    "            local_density = 0\n",
    "        \n",
    "        densities.append(local_density)\n",
    "    \n",
    "    densities = np.array(densities)\n",
    "    \n",
    "    # Normalize densities to [0, 1] for alpha mapping\n",
    "    if densities.max() > densities.min():\n",
    "        densities_norm = (densities - densities.min()) / (densities.max() - densities.min())\n",
    "    else:\n",
    "        densities_norm = np.ones_like(densities) * 0.5\n",
    "    \n",
    "    print(f\"{label}: density range {densities_norm.min():.3f} to {densities_norm.max():.3f}\")\n",
    "    \n",
    "    # Plot line segments with varying alpha\n",
    "    for i in range(len(x_line) - 1):\n",
    "        # Alpha based on average density of this segment\n",
    "        avg_density = (densities_norm[i] + densities_norm[i + 1]) / 2\n",
    "        alpha = 0.2 + 0.8 * avg_density  # Range from 0.2 to 1.0\n",
    "        \n",
    "        # Plot this segment\n",
    "        ax.plot([x_line[i], x_line[i + 1]], [y_line[i], y_line[i + 1]], \n",
    "                color=color, linewidth=8, alpha=alpha, \n",
    "                solid_capstyle='round', zorder=5)\n",
    "    \n",
    "    # Plot thin black centerline over the entire line\n",
    "    ax.plot(x_line, y_line, color='black', linewidth=1.5, alpha=1.0, \n",
    "            solid_capstyle='round', zorder=10)\n",
    "    \n",
    "    # Legend\n",
    "    ax.plot([], [], color=color, linewidth=8, alpha=0.6, label=label)\n",
    "    \n",
    "    return x_sorted, y_pred, model\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Define total length in km\n",
    "total_length_km = amite_line.geometry.iloc[0].length * 0.0003048\n",
    "\n",
    "# Loop through the four panels\n",
    "for idx, ax in zip([4, 9, 12, 20], axes.flatten()):\n",
    "    distance_km = int((idx * 5) * total_length_km / 115)\n",
    "    \n",
    "    # Define datasets with labels + colors\n",
    "    df_10, df_50, df_100, df_500 = cdf_data_frames[idx]\n",
    "    \n",
    "    # Build datasets list\n",
    "    datasets = [\n",
    "        (df_500, '500-yr', 'tab:blue'),\n",
    "        (df_100, '100-yr', 'tab:green'),\n",
    "        (df_50, '50-yr', 'tab:orange'),\n",
    "        (df_10, '10-yr', 'tab:red'),\n",
    "    ]\n",
    "    \n",
    "    lines_info = []\n",
    "    \n",
    "    for df, label, color in datasets:\n",
    "        x = df['depth_hydrologic'].values\n",
    "        y = df['depth_base_storm_raw'].values\n",
    "        \n",
    "        # Normalize weights to create a proper PDF (sum to 1)\n",
    "        raw_weights = df['prob'].values\n",
    "        weights = raw_weights / raw_weights.sum()  # This is now a proper PDF\n",
    "        \n",
    "        print(f\"{label}: PDF sum = {weights.sum():.6f}\")  # Should be 1.0\n",
    "        \n",
    "        # Plot weighted line\n",
    "        x_sorted, y_pred, model = plot_weighted_line(ax, x, y, weights, color, label)\n",
    "        lines_info.append((x_sorted, y_pred, model, label, color, df))\n",
    "    \n",
    "    # Set axis limits with proper buffering\n",
    "    all_x = np.concatenate([df['depth_hydrologic'].values for df, _, _ in datasets])\n",
    "    all_y = np.concatenate([df['depth_base_storm_raw'].values for df, _, _ in datasets])\n",
    "    \n",
    "    x_range = all_x.max() - all_x.min()\n",
    "    y_range = all_y.max() - all_y.min()\n",
    "    ax.set_xlim(all_x.min() - 0.1 * x_range, all_x.max() + 0.1 * x_range)\n",
    "    ax.set_ylim(all_y.min() - 0.1 * y_range, all_y.max() + 0.1 * y_range)\n",
    "    \n",
    "    # Set up the plot\n",
    "    ax.set_xlabel('Hydrologic-attributed Depth (ft)', fontsize=16)\n",
    "    ax.set_ylabel('Surge-attributed Depth (ft)', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.set_title(f'Design events at {distance_km:} km along the Amite River', fontsize=17)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(frameon=False, fontsize=12, loc='upper right')\n",
    "    \n",
    "    # Add rotated text annotations\n",
    "    for x_sorted, y_pred, model, label, color, df in lines_info:\n",
    "        x_mid = (x_sorted[0] + x_sorted[-1]) / 2\n",
    "        y_mid = np.interp(x_mid, x_sorted, y_pred)\n",
    "        \n",
    "        data_slope = (y_pred[-1] - y_pred[0]) / (x_sorted[-1] - x_sorted[0])\n",
    "        \n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "        bbox = ax.get_window_extent()\n",
    "        width_display = bbox.width\n",
    "        height_display = bbox.height\n",
    "        \n",
    "        x_scale = width_display / (xlim[1] - xlim[0])\n",
    "        y_scale = height_display / (ylim[1] - ylim[0])\n",
    "        \n",
    "        visual_slope = data_slope * (y_scale / x_scale)\n",
    "        angle_deg = np.degrees(np.arctan(visual_slope))\n",
    "        \n",
    "        avg_depth = np.mean(df['depth_hydrologic'].values) + np.mean(df['depth_base_storm_raw'].values)\n",
    "        offset = 0.3\n",
    "        \n",
    "        ax.text(\n",
    "            x_mid,\n",
    "            y_mid + offset,\n",
    "            f'Compound Flood Depth {avg_depth:.2f} ft',\n",
    "            color='black',\n",
    "            fontsize=12,\n",
    "            rotation=angle_deg,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"design_storms_weighted.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b96d2c3-795c-464c-a3a8-e6e8299bacf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Helper for regression fitting ---\n",
    "def fit_and_predict(x, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1,1), y)\n",
    "    order = np.argsort(x)\n",
    "    return x[order], model.predict(x.reshape(-1,1))[order], model\n",
    "\n",
    "# --- Data definitions ---\n",
    "# unpack the four DataFrames from cdf_data_frames[4]\n",
    "df_500, df_100, df_50, df_10 = cdf_data_frames[4]\n",
    "\n",
    "# build datasets list\n",
    "datasets = [\n",
    "    (df_500, '500-yr', 'tab:blue'),\n",
    "    (df_100, '100-yr', 'tab:green'),\n",
    "    (df_50,  '50-yr',  'tab:orange'),\n",
    "    (df_10,  '10-yr',  'tab:red'),\n",
    "]\n",
    "\n",
    "# --- Create side-by-side panels ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "# -------- Panel (a): Regression --------\n",
    "ax = axes[0]\n",
    "for df, label, color in datasets:\n",
    "    x = df['depth_hydrologic'].values\n",
    "    y = df['depth_base_storm_raw'].values\n",
    "    \n",
    "    # scatter\n",
    "    ax.scatter(x, y, color=color, alpha=0.4, s=20, label=f'{label} data')\n",
    "    \n",
    "    # regression line\n",
    "    x_sorted, y_pred, model = fit_and_predict(x, y)\n",
    "    ax.plot(x_sorted, y_pred, color=color, linewidth=2, linestyle='--', label=f'{label} fit')\n",
    "\n",
    "ax.set_xlabel('Hydrologic-attributed Depth (ft)', fontsize=12)\n",
    "ax.set_ylabel('Surge-attributed Depth (ft)', fontsize=12)\n",
    "ax.set_title('At 35 km Along the Amite River Transect', fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=10)\n",
    "\n",
    "# -------- Panel (b): KDE distributions --------\n",
    "ax = axes[1]\n",
    "sns.kdeplot(df_500['depth_hydrologic_percentage'], fill=True, alpha=0.3, label='500-yr', color='tab:blue', ax=ax)\n",
    "sns.kdeplot(df_100['depth_hydrologic_percentage'], fill=True, alpha=0.3, label='100-yr', color='tab:green', ax=ax)\n",
    "sns.kdeplot(df_50['depth_hydrologic_percentage'],  fill=True, alpha=0.3, label='50-yr',  color='tab:orange', ax=ax)\n",
    "sns.kdeplot(df_10['depth_hydrologic_percentage'],  fill=True, alpha=0.3, label='10-yr',  color='tab:red', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Hydrologic-attributed Depth (%)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "#ax.set_title('(b) Distribution of Hydrologic-attributed Depth', fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e56367-9ba6-4850-b91f-f3dff7828ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Helper for regression fitting ---\n",
    "def fit_and_predict(x, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1,1), y)\n",
    "    order = np.argsort(x)\n",
    "    return x[order], model.predict(x.reshape(-1,1))[order], model\n",
    "\n",
    "# --- Data definitions ---\n",
    "# unpack the four DataFrames from cdf_data_frames[4]\n",
    "df_500, df_100, df_50, df_10 = cdf_data_frames[12]\n",
    "\n",
    "# build datasets list\n",
    "datasets = [\n",
    "    (df_500, '500-yr', 'tab:blue'),\n",
    "    (df_100, '100-yr', 'tab:green'),\n",
    "    (df_50,  '50-yr',  'tab:orange'),\n",
    "    (df_10,  '10-yr',  'tab:red'),\n",
    "]\n",
    "\n",
    "# --- Create side-by-side panels ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "# -------- Panel (a): Regression --------\n",
    "ax = axes[0]\n",
    "for df, label, color in datasets:\n",
    "    x = df['depth_hydrologic'].values\n",
    "    y = df['depth_base_storm_raw'].values\n",
    "    \n",
    "    # scatter\n",
    "    ax.scatter(x, y, color=color, alpha=0.4, s=20, label=f'{label} data')\n",
    "    \n",
    "    # regression line\n",
    "    x_sorted, y_pred, model = fit_and_predict(x, y)\n",
    "    ax.plot(x_sorted, y_pred, color=color, linewidth=2, linestyle='--', label=f'{label} fit')\n",
    "\n",
    "ax.set_xlabel('Hydrologic-attributed Depth (ft)', fontsize=12)\n",
    "ax.set_ylabel('Surge-attributed Depth (ft)', fontsize=12)\n",
    "ax.set_title('At 35 km Along the Amite River Transect', fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=10)\n",
    "\n",
    "# -------- Panel (b): KDE distributions --------\n",
    "ax = axes[1]\n",
    "sns.kdeplot(df_500['depth_hydrologic_percentage'], fill=True, alpha=0.3, label='500-yr', color='tab:blue', ax=ax)\n",
    "sns.kdeplot(df_100['depth_hydrologic_percentage'], fill=True, alpha=0.3, label='100-yr', color='tab:green', ax=ax)\n",
    "sns.kdeplot(df_50['depth_hydrologic_percentage'],  fill=True, alpha=0.3, label='50-yr',  color='tab:orange', ax=ax)\n",
    "sns.kdeplot(df_10['depth_hydrologic_percentage'],  fill=True, alpha=0.3, label='10-yr',  color='tab:red', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Hydrologic-attributed Depth (%)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "#ax.set_title('(b) Distribution of Hydrologic-attributed Depth', fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99120dc1-6846-4714-b5b9-a7a4beb62646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Helper for regression fitting ---\n",
    "def fit_and_predict(x, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1,1), y)\n",
    "    order = np.argsort(x)\n",
    "    return x[order], model.predict(x.reshape(-1,1))[order], model\n",
    "\n",
    "# --- Data definitions ---\n",
    "# unpack the four DataFrames from cdf_data_frames[0]\n",
    "df_500, df_100, df_50, df_10 = cdf_data_frames[20]\n",
    "\n",
    "# build datasets list\n",
    "datasets = [\n",
    "    (df_500, '500-yr', 'tab:blue'),\n",
    "    (df_100, '100-yr', 'tab:green'),\n",
    "    (df_50,  '50-yr',  'tab:orange'),\n",
    "    (df_10,  '10-yr',  'tab:red'),\n",
    "]\n",
    "\n",
    "# --- Create side-by-side panels ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "# -------- Panel (a): Regression --------\n",
    "ax = axes[0]\n",
    "for df, label, color in datasets:\n",
    "    x = df['depth_hydrologic'].values\n",
    "    y = df['depth_base_storm_raw'].values\n",
    "    \n",
    "    # scatter\n",
    "    ax.scatter(x, y, color=color, alpha=0.4, s=20, label=f'{label} data')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Hydrologic-attributed Depth (ft)', fontsize=12)\n",
    "ax.set_ylabel('Surge-attributed Depth (ft)', fontsize=12)\n",
    "ax.set_title('At 35 km Along the Amite River Transect', fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=10)\n",
    "\n",
    "# -------- Panel (b): KDE distributions --------\n",
    "ax = axes[1]\n",
    "sns.kdeplot(df_500['depth_hydrologic_percentage'], fill=True, alpha=0.3, label='500-yr', color='tab:blue', ax=ax)\n",
    "sns.kdeplot(df_100['depth_hydrologic_percentage'], fill=True, alpha=0.3, label='100-yr', color='tab:green', ax=ax)\n",
    "sns.kdeplot(df_50['depth_hydrologic_percentage'],  fill=True, alpha=0.3, label='50-yr',  color='tab:orange', ax=ax)\n",
    "sns.kdeplot(df_10['depth_hydrologic_percentage'],  fill=True, alpha=0.3, label='10-yr',  color='tab:red', ax=ax)\n",
    "\n",
    "ax.set_xlabel('Hydrologic-attributed Depth (%)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "#ax.set_title('(b) Distribution of Hydrologic-attributed Depth', fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "ax.legend(frameon=False, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667d7d7f-6e43-4fcf-9d56-bde4eb05b129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Helper for regression fitting ---\n",
    "def fit_and_predict(x, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1,1), y)\n",
    "    order = np.argsort(x)\n",
    "    return x[order], model.predict(x.reshape(-1,1))[order], model\n",
    "\n",
    "# --- Simple weighted line function ---\n",
    "def plot_weighted_line(ax, x, y, weights, color, label, linestyle='-', linewidth=8):\n",
    "    \"\"\"\n",
    "    Plot regression line with constant thickness and varying opacity based on local weight density\n",
    "    \"\"\"\n",
    "    # Fit regression\n",
    "    x_sorted, y_pred, model = fit_and_predict(x, y)\n",
    "    \n",
    "    # Add 5% buffer to line extent\n",
    "    x_range = x_sorted.max() - x_sorted.min()\n",
    "    x_buffer = x_range * 0.05\n",
    "    x_start = x_sorted.min() - x_buffer\n",
    "    x_end = x_sorted.max() + x_buffer\n",
    "    \n",
    "    # Create many points along the line for smooth shading\n",
    "    n_points = 100\n",
    "    x_line = np.linspace(x_start, x_end, n_points)\n",
    "    y_line = model.predict(x_line.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate local density along the line\n",
    "    densities = []\n",
    "    for x_val in x_line:\n",
    "        # Find points within a window around this x value\n",
    "        window_size = x_range * 0.1  # 10% of data range\n",
    "        distances = np.abs(x - x_val)\n",
    "        nearby_mask = distances <= window_size\n",
    "        \n",
    "        if np.sum(nearby_mask) > 0:\n",
    "            # Sum of weights of nearby points (higher = more density)\n",
    "            local_density = np.sum(weights[nearby_mask])\n",
    "        else:\n",
    "            # If no points nearby, use minimum density\n",
    "            local_density = 0\n",
    "        \n",
    "        densities.append(local_density)\n",
    "    \n",
    "    densities = np.array(densities)\n",
    "    \n",
    "    # Normalize densities to [0, 1] for alpha mapping\n",
    "    if densities.max() > densities.min():\n",
    "        densities_norm = (densities - densities.min()) / (densities.max() - densities.min())\n",
    "    else:\n",
    "        densities_norm = np.ones_like(densities) * 0.5\n",
    "    \n",
    "    print(f\"{label}: density range {densities_norm.min():.3f} to {densities_norm.max():.3f}\")\n",
    "    \n",
    "    # Plot line segments with varying alpha\n",
    "    for i in range(len(x_line) - 1):\n",
    "        # Alpha based on average density of this segment\n",
    "        avg_density = (densities_norm[i] + densities_norm[i + 1]) / 2\n",
    "        alpha = 0.2 + 0.8 * avg_density  # Range from 0.2 to 1.0\n",
    "        \n",
    "        # Plot this segment\n",
    "        ax.plot([x_line[i], x_line[i + 1]], [y_line[i], y_line[i + 1]], \n",
    "                color=color, linewidth=linewidth, alpha=alpha, linestyle=linestyle,\n",
    "                solid_capstyle='round', zorder=5)\n",
    "    \n",
    "    # Plot thin black centerline over the entire line\n",
    "    ax.plot(x_line, y_line, color='black', linewidth=0.5, alpha=1.0, \n",
    "            linestyle=linestyle, solid_capstyle='round', zorder=10)\n",
    "    \n",
    "    # Legend (adjust linewidth for legend)\n",
    "    legend_linewidth = 12 if linewidth == 8 else 8\n",
    "    ax.plot([], [], color=color, linewidth=legend_linewidth, alpha=0.6, \n",
    "            linestyle=linestyle, label=label)\n",
    "    \n",
    "    return x_sorted, y_pred, model\n",
    "\n",
    "# --- Simple regression line function for top panel ---\n",
    "def plot_simple_regression_line(ax, x, y, weights, color, linestyle, linewidth=3):\n",
    "    \"\"\"\n",
    "    Plot simple regression line without weighted opacity effects\n",
    "    \"\"\"\n",
    "    # Fit regression\n",
    "    x_sorted, y_pred, model = fit_and_predict(x, y)\n",
    "    \n",
    "    # Add 5% buffer to line extent\n",
    "    x_range = x_sorted.max() - x_sorted.min()\n",
    "    x_buffer = x_range * 0.05\n",
    "    x_start = x_sorted.min() - x_buffer\n",
    "    x_end = x_sorted.max() + x_buffer\n",
    "    \n",
    "    # Create line points\n",
    "    x_line = np.linspace(x_start, x_end, 100)\n",
    "    y_line = model.predict(x_line.reshape(-1, 1))\n",
    "    \n",
    "    # Plot simple line\n",
    "    ax.plot(x_line, y_line, color=color, linewidth=linewidth, \n",
    "            linestyle=linestyle, alpha=0.8, solid_capstyle='round')\n",
    "    \n",
    "    return x_sorted, y_pred, model\n",
    "\n",
    "# --- Plot five panels: one wide top panel + 2x2 grid below ---\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "# Create subplot layout using gridspec for better control\n",
    "gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Top panel spans both columns\n",
    "ax_top = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# Four panels in 2x2 grid below\n",
    "axes = [fig.add_subplot(gs[1, 0]), fig.add_subplot(gs[1, 1]),\n",
    "        fig.add_subplot(gs[2, 0]), fig.add_subplot(gs[2, 1])]\n",
    "\n",
    "# Define total length in km\n",
    "total_length_km = amite_line.geometry.iloc[0].length * 0.0003048\n",
    "\n",
    "# Define locations and their corresponding line styles\n",
    "locations = [4, 9, 12, 20]\n",
    "location_styles = {\n",
    "    4: '-',      # Solid\n",
    "    9: '--',     # Dashed  \n",
    "    12: '-.',    # Dash-dot\n",
    "    20: ':'      # Dotted\n",
    "}\n",
    "\n",
    "# Colors for return periods\n",
    "rp_colors = {'10-yr': 'tab:red', '50-yr': 'tab:orange', '100-yr': 'tab:green', '500-yr': 'tab:blue'}\n",
    "\n",
    "# ================ TOP PANEL: All data combined ================\n",
    "print(\"Processing top panel...\")\n",
    "\n",
    "# Collect all data for axis limits\n",
    "all_x_combined = []\n",
    "all_y_combined = []\n",
    "\n",
    "# Plot all locations and return periods on the top panel\n",
    "for idx in locations:\n",
    "    distance_km = int((idx * 5) * total_length_km / 115)\n",
    "    linestyle = location_styles[idx]\n",
    "    \n",
    "    # Get datasets for this location\n",
    "    df_10, df_50, df_100, df_500 = cdf_data_frames[idx]\n",
    "    \n",
    "    datasets = [\n",
    "        (df_10, '10-yr', rp_colors['10-yr']),\n",
    "        (df_50, '50-yr', rp_colors['50-yr']),\n",
    "        (df_100, '100-yr', rp_colors['100-yr']),\n",
    "        (df_500, '500-yr', rp_colors['500-yr']),\n",
    "    ]\n",
    "    \n",
    "    for df, rp_label, color in datasets:\n",
    "        x = df['depth_hydrologic'].values\n",
    "        y = df['depth_base_storm_raw'].values\n",
    "        \n",
    "        # Normalize weights\n",
    "        raw_weights = df['prob'].values\n",
    "        weights = raw_weights / raw_weights.sum()\n",
    "        \n",
    "        # Plot simple regression line\n",
    "        plot_simple_regression_line(ax_top, x, y, weights, color, linestyle)\n",
    "        \n",
    "        # Collect data for axis limits\n",
    "        all_x_combined.extend(x)\n",
    "        all_y_combined.extend(y)\n",
    "\n",
    "# Set axis limits for top panel\n",
    "all_x_combined = np.array(all_x_combined)\n",
    "all_y_combined = np.array(all_y_combined)\n",
    "x_range = all_x_combined.max() - all_x_combined.min()\n",
    "y_range = all_y_combined.max() - all_y_combined.min()\n",
    "ax_top.set_xlim(all_x_combined.min() - 0.1 * x_range, all_x_combined.max() + 0.1 * x_range)\n",
    "ax_top.set_ylim(all_y_combined.min() - 0.1 * y_range, all_y_combined.max() + 0.1 * y_range)\n",
    "\n",
    "# Format top panel\n",
    "ax_top.set_xlabel('Hydrologic-attributed Depth (ft)', fontsize=16)\n",
    "ax_top.set_ylabel('Surge-attributed Depth (ft)', fontsize=16)\n",
    "ax_top.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax_top.set_title('Combined View: All Return Periods and Locations', fontsize=18, fontweight='bold', pad=20)\n",
    "ax_top.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Create custom legend for top panel\n",
    "legend_elements = []\n",
    "# Return period colors\n",
    "for rp in ['10-yr', '50-yr', '100-yr', '500-yr']:\n",
    "    legend_elements.append(plt.Line2D([0], [0], color=rp_colors[rp], lw=4, alpha=0.8, label=rp))\n",
    "\n",
    "# Location line styles  \n",
    "legend_elements.append(plt.Line2D([0], [0], color='gray', lw=3, linestyle='-', label='4km (solid)'))\n",
    "legend_elements.append(plt.Line2D([0], [0], color='gray', lw=3, linestyle='--', label='9km (dashed)'))\n",
    "legend_elements.append(plt.Line2D([0], [0], color='gray', lw=3, linestyle='-.', label='12km (dash-dot)'))\n",
    "legend_elements.append(plt.Line2D([0], [0], color='gray', lw=3, linestyle=':', label='20km (dotted)'))\n",
    "\n",
    "ax_top.legend(handles=legend_elements, frameon=False, fontsize=11, ncol=2, loc='upper right')\n",
    "\n",
    "# ============== ORIGINAL FOUR PANELS BELOW ================\n",
    "print(\"Processing individual panels...\")\n",
    "\n",
    "# Loop through the four panels\n",
    "for panel_idx, (idx, ax) in enumerate(zip(locations, axes)):\n",
    "    distance_km = int((idx * 5) * total_length_km / 115)\n",
    "    \n",
    "    print(f\"Processing panel {panel_idx + 1}: {distance_km}km\")\n",
    "    \n",
    "    # Define datasets with labels + colors\n",
    "    df_10, df_50, df_100, df_500 = cdf_data_frames[idx]\n",
    "    \n",
    "    # Build datasets list\n",
    "    datasets = [\n",
    "        (df_500, '500-yr', rp_colors['500-yr']),\n",
    "        (df_100, '100-yr', rp_colors['100-yr']),\n",
    "        (df_50, '50-yr', rp_colors['50-yr']),\n",
    "        (df_10, '10-yr', rp_colors['10-yr']),\n",
    "    ]\n",
    "    \n",
    "    lines_info = []\n",
    "    \n",
    "    for df, label, color in datasets:\n",
    "        x = df['depth_hydrologic'].values\n",
    "        y = df['depth_base_storm_raw'].values\n",
    "        \n",
    "        # Normalize weights to create a proper PDF (sum to 1)\n",
    "        raw_weights = df['prob'].values\n",
    "        weights = raw_weights / raw_weights.sum()  # This is now a proper PDF\n",
    "        \n",
    "        print(f\"{label}: PDF sum = {weights.sum():.6f}\")  # Should be 1.0\n",
    "        \n",
    "        # Plot weighted line\n",
    "        x_sorted, y_pred, model = plot_weighted_line(ax, x, y, weights, color, label)\n",
    "        lines_info.append((x_sorted, y_pred, model, label, color, df))\n",
    "    \n",
    "    # Set axis limits with proper buffering\n",
    "    all_x = np.concatenate([df['depth_hydrologic'].values for df, _, _ in datasets])\n",
    "    all_y = np.concatenate([df['depth_base_storm_raw'].values for df, _, _ in datasets])\n",
    "    \n",
    "    x_range = all_x.max() - all_x.min()\n",
    "    y_range = all_y.max() - all_y.min()\n",
    "    ax.set_xlim(all_x.min() - 0.1 * x_range, all_x.max() + 0.1 * x_range)\n",
    "    ax.set_ylim(all_y.min() - 0.1 * y_range, all_y.max() + 0.1 * y_range)\n",
    "    \n",
    "    # Set up the plot\n",
    "    ax.set_xlabel('Hydrologic-attributed Depth (ft)', fontsize=16)\n",
    "    ax.set_ylabel('Surge-attributed Depth (ft)', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.set_title(f'Design events at {distance_km:} km along the Amite River', fontsize=17)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(frameon=False, fontsize=12, loc='upper right')\n",
    "    \n",
    "    # Add rotated text annotations\n",
    "    for x_sorted, y_pred, model, label, color, df in lines_info:\n",
    "        x_mid = (x_sorted[0] + x_sorted[-1]) / 2\n",
    "        y_mid = np.interp(x_mid, x_sorted, y_pred)\n",
    "        \n",
    "        data_slope = (y_pred[-1] - y_pred[0]) / (x_sorted[-1] - x_sorted[0])\n",
    "        \n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "        bbox = ax.get_window_extent()\n",
    "        width_display = bbox.width\n",
    "        height_display = bbox.height\n",
    "        \n",
    "        x_scale = width_display / (xlim[1] - xlim[0])\n",
    "        y_scale = height_display / (ylim[1] - ylim[0])\n",
    "        \n",
    "        visual_slope = data_slope * (y_scale / x_scale)\n",
    "        angle_deg = np.degrees(np.arctan(visual_slope))\n",
    "        \n",
    "        avg_depth = np.mean(df['depth_hydrologic'].values) + np.mean(df['depth_base_storm_raw'].values)\n",
    "        offset = 0.3\n",
    "        \n",
    "        ax.text(\n",
    "            x_mid,\n",
    "            y_mid + offset,\n",
    "            f'Compound Flood Depth {avg_depth:.2f} ft',\n",
    "            color='black',\n",
    "            fontsize=12,\n",
    "            rotation=angle_deg,\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            bbox=dict(facecolor='white', alpha=0.0, edgecolor='none'),\n",
    "            zorder=11\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"design_storms_weighted_combined.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c162f41-cbf0-4203-8504-37e4bbc00494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Figure 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bc71f6-fa90-4d2f-adf9-632d519ac1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idx=9\n",
    "distance_km = (idx * 5) * total_length_km / 115 \n",
    "geo_frame_filtered_to_id = geo_frame[geo_frame['ras_id'] == closest_ras_ids[idx]]\n",
    "\n",
    "storm_data_all_uncert_by_id = storm_data_all_uncert[(storm_data_all_uncert['ras_id'] == \n",
    "closest_ras_ids[idx])]\n",
    "storm_data_surge_uncert_by_id  = storm_data_surge_uncert[(storm_data_surge_uncert['ras_id'] == \n",
    "closest_ras_ids[idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ee8519a-8ed7-4187-a8f0-f1f09a48cc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'storm_id' and calculate the mean of 'depth_raw', sum of 'prob', and return period\n",
    "storm_data_aggregated = storm_data_all_uncert_by_id.groupby('storm_id').agg(\n",
    "    mean_depth=('depth_raw', 'mean'),\n",
    "    total_prob=('prob', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "storm_data_aggregated = storm_data_aggregated.sort_values(by='mean_depth')\n",
    "storm_data_aggregated['cum_prob'] = storm_data_aggregated['total_prob'].cumsum()\n",
    "\n",
    "# Calculate return period\n",
    "storm_data_aggregated['annualized_cdf_val'] = calc_annualized_cdf_val(storm_data_aggregated, recurrence_rate, 'cum_prob')\n",
    "\n",
    "\n",
    "storm_data_aggregated['return_period'] = 1 /(1- storm_data_aggregated['annualized_cdf_val'])\n",
    "\n",
    "display(storm_data_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27eb28ee-22a0-478b-82ed-bfbcba68e361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Set publication-quality style\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "#    'font.family': 'serif',\n",
    "#    'font.serif': ['Times New Roman', 'DejaVu Serif'],\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 15,\n",
    "    'axes.labelsize': 15,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.dpi': 300,\n",
    "    'axes.linewidth': 1.0,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'axes.axisbelow': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.linewidth': 0.5,\n",
    "})\n",
    "\n",
    "# Calculate the 500-year storm WSE\n",
    "closest_row = storm_data_all_uncert_by_id.iloc[(storm_data_all_uncert_by_id['return_period'] - 500).abs().argsort()[:1]]\n",
    "storm_data_all_500_wse = closest_row['depth_raw'].values[0] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "\n",
    "# Get unique storm IDs\n",
    "unique_storm_ids = cdf_data_frames[idx][3]['storm_id'].unique()\n",
    "\n",
    "# Collect data for each storm\n",
    "storm_data_collection = []\n",
    "\n",
    "total_prob = (storm_data_surge_uncert[storm_data_surge_uncert['storm_id'].isin(unique_storm_ids)]\n",
    "              .groupby('storm_id')['prob'].first().sum())\n",
    "\n",
    "for storm_id in unique_storm_ids:\n",
    "    # Get compound flooding data (with rainfall)\n",
    "    compound_data = storm_data_all_uncert[\n",
    "        (storm_data_all_uncert['ras_id'] == closest_ras_ids[idx]) & \n",
    "        (storm_data_all_uncert['storm_id'] == storm_id)\n",
    "    ]\n",
    "    \n",
    "    # Get surge-only data (without rainfall)\n",
    "    surge_data = storm_data_surge_uncert[\n",
    "        (storm_data_surge_uncert['ras_id'] == closest_ras_ids[idx]) & \n",
    "        (storm_data_surge_uncert['storm_id'] == storm_id)\n",
    "    ]\n",
    "    \n",
    "    if len(compound_data) > 5 and len(surge_data) > 0:  # Require sufficient data\n",
    "        compound_wse = compound_data['depth_raw'] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "        surge_wse = surge_data['depth_raw'] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "        \n",
    "        storm_info = {\n",
    "            'storm_id': storm_id,\n",
    "            'prob':  float(surge_data['prob'].values[0]/total_prob),\n",
    "            'compound_wse_values': compound_wse.values*0.3048,\n",
    "            'compound_prob_values': compound_data['prob'].values,\n",
    "            'compound_wse_min': np.min(compound_wse)*0.3048,\n",
    "            'compound_mean': np.mean(compound_wse)*0.3048,\n",
    "            'surge_mean': np.mean(surge_wse)*0.3048,\n",
    "            'surge_wse_values': surge_wse.values*0.3048,\n",
    "            'hydrologic_contribution_percent': (np.mean(compound_wse) - np.mean(surge_wse)) / np.mean(compound_wse) * 100\n",
    "        }\n",
    "        storm_data_collection.append(storm_info)\n",
    "\n",
    "# Sort storms by their mean compound WSE\n",
    "storm_data_collection.sort(key=lambda x: x['compound_wse_min'])\n",
    "\n",
    "# ========================\n",
    "# Create design storm sets based on surge means\n",
    "# ========================\n",
    "percentiles = [0, 25, 50, 75, 100]\n",
    "percentile_ranges = {}\n",
    "\n",
    "# Extract storm_id and surge_mean\n",
    "surge_means = [(storm['storm_id'], storm['surge_mean']) for storm in storm_data_collection]\n",
    "surge_means.sort(key=lambda x: x[1])  # sort by surge_mean\n",
    "surge_values = [x[1] for x in surge_means]\n",
    "\n",
    "# Compute cutoff values\n",
    "cutoffs = np.percentile(surge_values, percentiles)\n",
    "\n",
    "# Assign storms to bins\n",
    "for i in range(len(percentiles)-1):\n",
    "    low, high = cutoffs[i], cutoffs[i+1]\n",
    "    storms_in_range = [sid for sid, val in surge_means if low <= val <= high]\n",
    "    percentile_ranges[f\"{percentiles[i]}-{percentiles[i+1]}\"] = storms_in_range\n",
    "\n",
    "# Create a mapping from storm_id to design set\n",
    "storm_to_set = {}\n",
    "set_names = ['Storm Set 1', 'Storm Set 2', 'Storm Set 3', 'Storm Set 4', 'Storm Set 5']\n",
    "\n",
    "##########\n",
    "###########\n",
    "storm_probs = [(f\"{storm['storm_id']}\", storm['prob'], sum(storm['prob'] for storm in storm_data_collection[:i+1])) for i, storm in enumerate(storm_data_collection)]\n",
    "# Initialize groups\n",
    "groups = {\n",
    "    '0-0.2': [],\n",
    "    '0.2-0.4': [],\n",
    "    '0.4-0.6': [],\n",
    "    '0.6-0.8': [],\n",
    "    '0.8-1.0': []\n",
    "}\n",
    "\n",
    "# Group storms based on cumulative probability\n",
    "for storm_id, prob, cum_prob in storm_probs:\n",
    "    if cum_prob <= 0.2:\n",
    "        groups['0-0.2'].append((storm_id, prob, cum_prob))\n",
    "    elif cum_prob <= 0.4:\n",
    "        groups['0.2-0.4'].append((storm_id, prob, cum_prob))\n",
    "    elif cum_prob <= 0.6:\n",
    "        groups['0.4-0.6'].append((storm_id, prob, cum_prob))\n",
    "    elif cum_prob <= 0.8:\n",
    "        groups['0.6-0.8'].append((storm_id, prob, cum_prob))\n",
    "    else:\n",
    "        groups['0.8-1.0'].append((storm_id, prob, cum_prob))\n",
    "\n",
    "# Create storm_id to group number mapping\n",
    "storm_to_set = {}\n",
    "group_mapping = {\n",
    "    '0-0.2': 0,\n",
    "    '0.2-0.4': 1,\n",
    "    '0.4-0.6': 2,\n",
    "    '0.6-0.8': 3,\n",
    "    '0.8-1.0': 4\n",
    "}\n",
    "\n",
    "for group_name, storms in groups.items():\n",
    "    group_num = group_mapping[group_name]\n",
    "    for storm_id, prob, cum_prob in storms:\n",
    "        storm_to_set[int(storm_id)] = group_num\n",
    "\n",
    "print(\"Storm ID to Group Number mapping:\")\n",
    "print(storm_to_set)\n",
    "#################\n",
    "##################\n",
    "\n",
    "# Create the main figure with professional layout\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[20, 1], hspace=0.01)\n",
    "\n",
    "# Main plot\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "\n",
    "# Parameters\n",
    "bar_width = 0.75\n",
    "storm_positions = np.arange(len(storm_data_collection))\n",
    "\n",
    "# ================================\n",
    "# Refined violin plot colors with blue reference\n",
    "# ================================\n",
    "\n",
    "# Main plot colors\n",
    "surge_color = 'tab:blue'            # Keep your reference blue\n",
    "compound_color = 'tab:purple'       # Deep purple, contrasts with blue but still professional\n",
    "\n",
    "# Whiskers and outlines\n",
    "whisker_color = 'black'             # Clear and neutral\n",
    "violin_outline_color = 'black'      # Defines violin shape\n",
    "\n",
    "# Optional reference/median line\n",
    "reference_color = '#777777'         # Pops for medians or key lines\n",
    "\n",
    "# Design set or group colors\n",
    "set_colors = ['tab:blue', 'tab:green', 'tab:purple', 'tab:red']  # Cohesive and distinct\n",
    "\n",
    "# Reference line for 500-year WSE\n",
    "ax_main.axhline(y=storm_data_all_500_wse*0.3048, color=reference_color, linestyle='--', \n",
    "                linewidth=2.5, label=f'500-year Return Period WSE ({storm_data_all_500_wse*0.3048:.1f} m)', \n",
    "                alpha=0.9, zorder=10)\n",
    "\n",
    "# Plot each storm\n",
    "for i, storm_info in enumerate(storm_data_collection):\n",
    "    storm_id = storm_info['storm_id']\n",
    "    compound_wse_values = storm_info['compound_wse_values']\n",
    "    prob_values = storm_info['prob']\n",
    "    surge_mean = storm_info['surge_mean']\n",
    "    compound_mean = storm_info['compound_mean']\n",
    "    \n",
    "    x_pos = storm_positions[i]\n",
    "    \n",
    "    # 1. Plot surge-only bar (baseline contribution)\n",
    "    surge_bar = ax_main.bar(x_pos, np.min(compound_wse_values), width=bar_width, \n",
    "                           color=surge_color, alpha=0.8, edgecolor='white', linewidth=0.5,\n",
    "                           label='Storm Surge Component' if i == 0 else \"\")\n",
    "    \n",
    "    # 2. Enhanced violin plot for compound WSE variability\n",
    "    violin_data = compound_wse_values\n",
    "    violin_weights = prob_values \n",
    "    \n",
    "    if len(violin_data) > 3:\n",
    "        # Calculate statistics\n",
    "        p25 = np.percentile(violin_data, 25)\n",
    "        p50 = np.percentile(violin_data, 50)  # median\n",
    "        p75 = np.percentile(violin_data, 75)\n",
    "        data_min = np.min(violin_data)\n",
    "        data_max = np.max(violin_data)\n",
    "        \n",
    "        # Create density estimation\n",
    "        kde = gaussian_kde(violin_data)\n",
    "        density_range = np.linspace(data_min, data_max, 150)\n",
    "        density_values = kde(density_range)\n",
    "        \n",
    "        # Normalize density for violin width\n",
    "        max_density = np.max(density_values)\n",
    "        density_norm = (density_values / max_density) * (bar_width / 3.5)\n",
    "        \n",
    "        # Plot violin (compound flooding variability)\n",
    "        ax_main.fill_betweenx(density_range, \n",
    "                             x_pos - density_norm, \n",
    "                             x_pos + density_norm,\n",
    "                             alpha=0.7, color=compound_color, \n",
    "                             label='Variable Hydrologic Component (Rainfall Driven)' if i == 0 else \"\")\n",
    "        \n",
    "        # Add violin outline\n",
    "        ax_main.plot(x_pos - density_norm, density_range, color=violin_outline_color, linewidth=0.8, alpha=0.9)\n",
    "        ax_main.plot(x_pos + density_norm, density_range, color=violin_outline_color, linewidth=0.8, alpha=0.9)\n",
    "        \n",
    "        # Add statistical markers\n",
    "        marker_width = bar_width / 5\n",
    "        # Median line (white for visibility)\n",
    "        ax_main.plot([x_pos - marker_width, x_pos + marker_width], \n",
    "                    [p50, p50], 'white', linewidth=2.75, alpha=0.95)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Whiskers (connecting to extremes)\n",
    "        ax_main.plot([x_pos, x_pos], [data_min, p25], color= whisker_color, \n",
    "                    linewidth=1.2, alpha=0.7, linestyle='-')\n",
    "        ax_main.plot([x_pos, x_pos], [p75, data_max], color=whisker_color, \n",
    "                    linewidth=1.2, alpha=0.7, linestyle='-')\n",
    "\n",
    "        # IQR box (subtle)\n",
    "        iqr_width = bar_width / 8\n",
    "        ax_main.plot([x_pos - iqr_width, x_pos + iqr_width], \n",
    "                    [p25, p25], 'white', linewidth=1.75, alpha=0.95)\n",
    "        ax_main.plot([x_pos - iqr_width, x_pos + iqr_width], \n",
    "                    [p75, p75], 'white', linewidth=1.75, alpha=0.95)\n",
    "\n",
    "        # Add min/max markers\n",
    "        marker_width = bar_width / 5\n",
    "        ax_main.plot([x_pos - marker_width, x_pos + marker_width], \n",
    "               [data_min, data_min], violin_outline_color, linewidth=2, alpha=0.7)\n",
    "        ax_main.plot([x_pos - marker_width, x_pos + marker_width], \n",
    "               [data_max, data_max], violin_outline_color, linewidth=2, alpha=0.7)\n",
    "\n",
    "\n",
    "\n",
    "# Customize main plot\n",
    "ax_main.set_ylabel('Water Surface Elevation (m NAVD88)') #fontweight='bold'\n",
    "ax_main.set_title('Amite River at 45 km', pad=15)\n",
    "\n",
    "# Set x-axis (will be handled by subplot below)\n",
    "ax_main.set_xticks(storm_positions)\n",
    "ax_main.set_xticklabels([])  # Remove x-labels from main plot\n",
    "\n",
    "# Professional legend\n",
    "legend = ax_main.legend(loc='upper left', frameon=True, fancybox=False, \n",
    "                       shadow=False, edgecolor='black', facecolor='white', \n",
    "                       framealpha=0.95, borderpad=0.8)\n",
    "legend.get_frame().set_linewidth(0.8)\n",
    "\n",
    "# Enhanced grid\n",
    "ax_main.grid(True, alpha=0.4, linestyle='-', linewidth=0.5)\n",
    "ax_main.set_axisbelow(True)\n",
    "\n",
    "# ========================\n",
    "# Design Storm Set Classification (bottom panel) - FIXED\n",
    "# ========================\n",
    "ax_sets = fig.add_subplot(gs[1])\n",
    "\n",
    "# Get positions for each design storm set\n",
    "set_positions = {}\n",
    "for i, storm_info in enumerate(storm_data_collection):\n",
    "    storm_id = storm_info['storm_id']\n",
    "    set_index = storm_to_set.get(storm_id, 0)\n",
    "    if set_index not in set_positions:\n",
    "        set_positions[set_index] = []\n",
    "    set_positions[set_index].append(storm_positions[i])\n",
    "\n",
    "# Draw design storm set classifications with proper positioning\n",
    "for set_index, positions in set_positions.items():\n",
    "    if len(positions) > 0:\n",
    "        color = set_colors[set_index % len(set_colors)]\n",
    "        \n",
    "        # Calculate proper rectangle bounds\n",
    "        left_pos = min(positions) - bar_width/2\n",
    "        right_pos = max(positions) + bar_width/2\n",
    "        rect_width = right_pos - left_pos\n",
    "        ##\n",
    "\n",
    "        rect_height = 0.16  # your choice\n",
    "        rect_y = -rect_height/2  #\n",
    "        # Background rectangle - properly sized around the text area\n",
    "        rect = Rectangle((left_pos, rect_y), rect_width, rect_height, \n",
    "                        facecolor=color, alpha=0.15, edgecolor=color, \n",
    "                        linewidth=1.5)\n",
    "        ax_sets.add_patch(rect)\n",
    "        \n",
    "        # Set label - centered properly\n",
    "        center_pos = np.mean(positions)\n",
    "        ax_sets.text(center_pos, 0, set_names[set_index], \n",
    "                    ha='center', va='center', fontweight='bold',\n",
    "                    fontsize=10, color=color)\n",
    "\n",
    "# Configure bottom panel\n",
    "ax_sets.set_xlim(ax_main.get_xlim())\n",
    "ax_sets.set_ylim(-0.1,0.1)\n",
    "ax_sets.set_xlabel('Storm ID (JPM Storm Suite)') #fontweight='bold'\n",
    "\n",
    "# Storm ID labels\n",
    "storm_labels = [f\"{storm['storm_id']}\" for storm in storm_data_collection]\n",
    "ax_sets.set_xticks(storm_positions)\n",
    "ax_sets.set_xticklabels(storm_labels, rotation=45, ha='right')\n",
    "ax_sets.set_yticks([])\n",
    "\n",
    "# Remove spines for clean look\n",
    "for spine in ax_sets.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# REMOVED the separator line that was causing the grey line issue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.12)  # Extra space for rotated labels\n",
    "plt.savefig(\"design_storm_500_yr.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094a11e6-c613-41f8-93b4-1ac901b37c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Calculate the 500-year storm WSE\n",
    "closest_row = storm_data_all_uncert_by_id.iloc[(storm_data_all_uncert_by_id['return_period'] - 500).abs().argsort()[:1]]\n",
    "storm_data_all_500_wse = closest_row['depth_raw'].values[0] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "\n",
    "# Get unique storm IDs\n",
    "unique_storm_ids = cdf_data_frames[idx][3]['storm_id'].unique()\n",
    "\n",
    "# Collect data for each storm\n",
    "storm_data_collection = []\n",
    "\n",
    "for storm_id in unique_storm_ids:\n",
    "    # Get compound flooding data (with rainfall)\n",
    "    compound_data = storm_data_all_uncert[\n",
    "        (storm_data_all_uncert['ras_id'] == closest_ras_ids[idx]) & \n",
    "        (storm_data_all_uncert['storm_id'] == storm_id)\n",
    "    ]\n",
    "    \n",
    "    # Get surge-only data (without rainfall)\n",
    "    surge_data = storm_data_surge_uncert[\n",
    "        (storm_data_surge_uncert['ras_id'] == closest_ras_ids[idx]) & \n",
    "        (storm_data_surge_uncert['storm_id'] == storm_id)\n",
    "    ]\n",
    "    \n",
    "    if len(compound_data) > 5 and len(surge_data) > 0:  # Require sufficient data\n",
    "        compound_wse = compound_data['depth_raw'] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "        surge_wse = surge_data['depth_raw'] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "        \n",
    "        storm_info = {\n",
    "            'storm_id': storm_id,\n",
    "            'compound_wse_values': compound_wse.values,\n",
    "            'compound_wse_min': np.min(compound_wse),\n",
    "            'compound_mean': np.mean(compound_wse),\n",
    "            'surge_mean': np.mean(surge_wse),\n",
    "            'surge_wse_values': surge_wse.values,\n",
    "            'hydrologic_contribution_percent': np.mean(compound_wse) - np.mean(surge_wse) / np.mean(compound_wse) * 100\n",
    "        }\n",
    "        storm_data_collection.append(storm_info)\n",
    "\n",
    "# Sort storms by their mean compound WSE\n",
    "storm_data_collection.sort(key=lambda x: x['compound_wse_min'])\n",
    "\n",
    "# ========================\n",
    "# Create design storm sets based on surge means\n",
    "# ========================\n",
    "percentiles = [0, 25, 50, 75, 100]\n",
    "percentile_ranges = {}\n",
    "\n",
    "# Extract storm_id and surge_mean\n",
    "surge_means = [(storm['storm_id'], storm['surge_mean']) for storm in storm_data_collection]\n",
    "surge_means.sort(key=lambda x: x[1])  # sort by surge_mean\n",
    "surge_values = [x[1] for x in surge_means]\n",
    "\n",
    "# Compute cutoff values\n",
    "cutoffs = np.percentile(surge_values, percentiles)\n",
    "\n",
    "# Assign storms to bins\n",
    "for i in range(len(percentiles)-1):\n",
    "    low, high = cutoffs[i], cutoffs[i+1]\n",
    "    storms_in_range = [sid for sid, val in surge_means if low <= val <= high]\n",
    "    percentile_ranges[f\"{percentiles[i]}-{percentiles[i+1]}\"] = storms_in_range\n",
    "\n",
    "# Create a mapping from storm_id to design set\n",
    "storm_to_set = {}\n",
    "set_names = ['Design Storm Set 1', 'Design Storm Set 2', 'Design Storm Set 3', 'Design Storm Set 4']\n",
    "\n",
    "for i, (percentile_range, storm_list) in enumerate(percentile_ranges.items()):\n",
    "    for storm_id in storm_list:\n",
    "        storm_to_set[storm_id] = i\n",
    "\n",
    "# Create the plot with more space at the bottom for the enhanced brackets\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "\n",
    "# Plot parameters\n",
    "bar_width = 0.6\n",
    "storm_positions = np.arange(len(storm_data_collection))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(storm_data_collection)))\n",
    "\n",
    "# Plot each storm\n",
    "for i, storm_info in enumerate(storm_data_collection):\n",
    "    storm_id = storm_info['storm_id']\n",
    "    compound_wse_values = storm_info['compound_wse_values']\n",
    "    surge_mean = storm_info['surge_mean']\n",
    "    compound_mean = storm_info['compound_mean']\n",
    "    \n",
    "    x_pos = storm_positions[i]\n",
    "    \n",
    "    # 1. Plot surge-only bar (from 0 to surge mean)\n",
    "    surge_bar = ax.bar(x_pos, np.min(compound_wse_values), width=bar_width, \n",
    "                      color='lightblue', alpha=0.8, edgecolor='navy', linewidth=1,\n",
    "                      label='Surge Contribution' if i == 0 else \"\")\n",
    "    \n",
    "    # 2. Create a box-and-whisker style plot for compound WSE variability\n",
    "    # But position it ABOVE the surge contribution to avoid overlap\n",
    "    violin_data = compound_wse_values\n",
    "    \n",
    "    if len(violin_data) > 3:\n",
    "        # Calculate percentiles for the box plot style\n",
    "        p25 = np.percentile(violin_data, 25)\n",
    "        p50 = np.percentile(violin_data, 50)  # median\n",
    "        p75 = np.percentile(violin_data, 75)\n",
    "        p10 = np.percentile(violin_data, 10)\n",
    "        p90 = np.percentile(violin_data, 90)\n",
    "        data_min = np.min(violin_data)\n",
    "        data_max = np.max(violin_data)\n",
    "        \n",
    "        # Create a density plot positioned above the surge bar\n",
    "        kde = gaussian_kde(violin_data)\n",
    "        density_range = np.linspace(surge_mean, data_max, 100)  # Start from surge mean\n",
    "        density_values = kde(density_range)\n",
    "        \n",
    "        # Normalize density for width (scale to bar_width)\n",
    "        max_density = np.max(density_values)\n",
    "        density_norm = (density_values / max_density) * (bar_width / 3)  # Narrower than bar\n",
    "        \n",
    "        # Plot the density curve as filled area ABOVE the surge bar\n",
    "        ax.fill_betweenx(density_range, \n",
    "                        x_pos - density_norm, \n",
    "                        x_pos + density_norm,\n",
    "                        alpha=0.7, color='red', \n",
    "                        label='Hydrologic variability' if i == 0 else \"\")\n",
    "        \n",
    "        # Add outline for the density\n",
    "        ax.plot(x_pos - density_norm, density_range, 'darkred', linewidth=1.5, alpha=0.8)\n",
    "        ax.plot(x_pos + density_norm, density_range, 'darkred', linewidth=1.5, alpha=0.8)\n",
    "        \n",
    "        # Add key percentile markers as horizontal lines\n",
    "        percentile_width = bar_width / 4\n",
    "        ax.plot([x_pos - percentile_width, x_pos + percentile_width], \n",
    "               [p50, p50], 'white', linewidth=2, alpha=0.9)  # Median line\n",
    "        \n",
    "        # Add whiskers showing the range\n",
    "        ax.plot([x_pos, x_pos], [data_min, surge_mean], 'red', linewidth=2, alpha=0.6, linestyle=':')\n",
    "        ax.plot([x_pos, x_pos], [data_max, p75], 'red', linewidth=2, alpha=0.6, linestyle=':')\n",
    "        \n",
    "        # Add min/max markers\n",
    "        marker_width = bar_width / 6\n",
    "        ax.plot([x_pos - marker_width, x_pos + marker_width], \n",
    "               [data_min, data_min], 'red', linewidth=2, alpha=0.7)\n",
    "        ax.plot([x_pos - marker_width, x_pos + marker_width], \n",
    "               [data_max, data_max], 'red', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Mark 500-year WSE line\n",
    "ax.axhline(y=storm_data_all_500_wse, color='orange', linestyle='--', linewidth=3,\n",
    "           label=f'500-Year WSE ({storm_data_all_500_wse:.2f} ft)', alpha=0.9)\n",
    "\n",
    "# ========================\n",
    "# Create enhanced design storm set brackets below x-axis\n",
    "# ========================\n",
    "# Get the y-axis limits to position the brackets below the plot\n",
    "y_min, y_max = ax.get_ylim()\n",
    "y_range = y_max - y_min\n",
    "\n",
    "# Define bracket styling parameters\n",
    "bracket_height = y_range * 0.015  # Height of bracket lines\n",
    "bracket_spacing = y_range * 0.03  # Spacing between bracket levels\n",
    "bracket_start_offset = y_range * 0.05  # Distance below x-axis to start brackets\n",
    "\n",
    "# Colors for different design storm sets\n",
    "set_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "\n",
    "# Get positions for each design storm set\n",
    "set_positions = {}\n",
    "for i, storm_info in enumerate(storm_data_collection):\n",
    "    storm_id = storm_info['storm_id']\n",
    "    set_index = storm_to_set.get(storm_id, 0)\n",
    "    if set_index not in set_positions:\n",
    "        set_positions[set_index] = []\n",
    "    set_positions[set_index].append(storm_positions[i])\n",
    "\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Storm ID (from the 645 JPM storm set)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Water Surface Elevation (WSE) ft', fontsize=14, fontweight='bold')\n",
    "ax.set_title('At 45 km along the Amite River', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Set x-axis labels to storm IDs\n",
    "storm_labels = [f\"Storm\\n{storm['storm_id']}\" for storm in storm_data_collection]\n",
    "ax.set_xticks(storm_positions)\n",
    "ax.set_xticklabels(storm_labels, fontsize=10)\n",
    "\n",
    "# Add legend for the main plot elements\n",
    "ax.legend(loc='upper left', fontsize=12, frameon=True, fancybox=True, shadow=True)\n",
    "\n",
    "# Add grid (only for the main plot area)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Style improvements\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.2)\n",
    "ax.spines['bottom'].set_linewidth(1.2)\n",
    "\n",
    "# Ensure the x-axis spine is visible and create space for brackets below the plot\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "\n",
    "# Create a separate area below the main plot for brackets using matplotlib's transform system\n",
    "trans = ax.get_xaxis_transform()\n",
    "\n",
    "# Draw brackets in the transform coordinate system (x in data coords, y in axes coords)\n",
    "for set_index, positions in set_positions.items():\n",
    "    if len(positions) > 0:\n",
    "        left_pos = min(positions) - bar_width/2\n",
    "        right_pos = max(positions) + bar_width/2\n",
    "        center_pos = np.mean(positions)\n",
    "        \n",
    "        color = set_colors[set_index % len(set_colors)]\n",
    "        \n",
    "        # Bracket positions in axes coordinates (below the plot)\n",
    "        bracket_y_top = -0.06  # 8% below the plot\n",
    "        bracket_y_bottom = -0.08  # 10% below the plot\n",
    "        \n",
    "        # Main horizontal line\n",
    "        ax.plot([left_pos, right_pos], [bracket_y_bottom, bracket_y_bottom], \n",
    "               color=color, linewidth=3, solid_capstyle='round', transform=trans, clip_on=False)\n",
    "        \n",
    "        # Left vertical connector\n",
    "        ax.plot([left_pos, left_pos], [bracket_y_top, bracket_y_bottom], \n",
    "               color=color, linewidth=2.5, solid_capstyle='round', transform=trans, clip_on=False)\n",
    "        \n",
    "        # Right vertical connector  \n",
    "        ax.plot([right_pos, right_pos], [bracket_y_top, bracket_y_bottom], \n",
    "               color=color, linewidth=2.5, solid_capstyle='round', transform=trans, clip_on=False)\n",
    "        \n",
    "        # Add small decorative elements at the ends\n",
    "        end_cap_size = bar_width * 0.1\n",
    "        ax.plot([left_pos - end_cap_size, left_pos + end_cap_size], \n",
    "               [bracket_y_top, bracket_y_top], color=color, linewidth=2.5, transform=trans, clip_on=False)\n",
    "        ax.plot([right_pos - end_cap_size, right_pos + end_cap_size], \n",
    "               [bracket_y_top, bracket_y_top], color=color, linewidth=2.5, transform=trans, clip_on=False)\n",
    "        \n",
    "        # Add label\n",
    "        label_y = -0.1  # 14% below the plot\n",
    "        ax.text(center_pos, label_y, \n",
    "               set_names[set_index], \n",
    "               ha='center', va='center', \n",
    "               fontsize=11, fontweight='bold',\n",
    "               color=color, transform=trans,\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', \n",
    "                        edgecolor=color, alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# Create a summary table\n",
    "# ========================\n",
    "print(\"=\" * 90)\n",
    "print(\"STORM-BY-STORM WSE DECOMPOSITION ANALYSIS\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"500-Year Target WSE: {storm_data_all_500_wse:.3f} ft\")\n",
    "print()\n",
    "print(\"Storm ID | Design Set | Surge Mean | Compound Mean | Rainfall Contrib | Rainfall Range | Exceeds 500-yr\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "total_realizations = 0\n",
    "total_exceeds_500 = 0\n",
    "\n",
    "for storm_info in storm_data_collection:\n",
    "    storm_id = storm_info['storm_id']\n",
    "    surge_mean = storm_info['surge_mean']\n",
    "    compound_mean = storm_info['compound_mean']\n",
    "    compound_values = storm_info['compound_wse_values']\n",
    "    set_index = storm_to_set.get(storm_id, 0)\n",
    "    \n",
    "    rainfall_contrib = compound_mean - surge_mean\n",
    "    rainfall_range = f\"{np.min(compound_values):.2f}-{np.max(compound_values):.2f}\"\n",
    "    \n",
    "    exceeds_500 = np.sum(compound_values >= storm_data_all_500_wse)\n",
    "    total_exceeds_500 += exceeds_500\n",
    "    total_realizations += len(compound_values)\n",
    "    \n",
    "    print(f\"   {storm_id:4d}  |    Set {set_index+1}    |   {surge_mean:7.2f}  |    {compound_mean:8.2f}  |      {rainfall_contrib:6.2f}    | {rainfall_range:>13} |  {exceeds_500:3d}/{len(compound_values):3d} ({exceeds_500/len(compound_values)*100:4.1f}%)\")\n",
    "\n",
    "print(\"-\" * 105)\n",
    "print(f\"OVERALL: {total_exceeds_500}/{total_realizations} realizations exceed 500-year WSE ({total_exceeds_500/total_realizations*100:.1f}%)\")\n",
    "\n",
    "# Key insights\n",
    "max_surge_storm = max(storm_data_collection, key=lambda x: x['surge_mean'])\n",
    "max_compound_storm = max(storm_data_collection, key=lambda x: x['compound_mean'])\n",
    "max_rainfall_contrib_storm = max(storm_data_collection, key=lambda x: x['compound_mean'] - x['surge_mean'])\n",
    "\n",
    "print(f\"\\nKEY INSIGHTS:\")\n",
    "print(f\"• Highest surge contribution: Storm {max_surge_storm['storm_id']} ({max_surge_storm['surge_mean']:.2f} ft)\")\n",
    "print(f\"• Highest compound WSE: Storm {max_compound_storm['storm_id']} ({max_compound_storm['compound_mean']:.2f} ft)\")\n",
    "print(f\"• Largest rainfall contribution: Storm {max_rainfall_contrib_storm['storm_id']} ({max_rainfall_contrib_storm['compound_mean'] - max_rainfall_contrib_storm['surge_mean']:.2f} ft)\")\n",
    "\n",
    "# Calculate overall statistics\n",
    "all_surge_means = [storm['surge_mean'] for storm in storm_data_collection]\n",
    "all_compound_means = [storm['compound_mean'] for storm in storm_data_collection]\n",
    "all_rainfall_contribs = [storm['compound_mean'] - storm['surge_mean'] for storm in storm_data_collection]\n",
    "\n",
    "print(f\"\\nOVERALL STATISTICS:\")\n",
    "print(f\"• Average surge contribution: {np.mean(all_surge_means):.2f} ± {np.std(all_surge_means):.2f} ft\")\n",
    "print(f\"• Average compound WSE: {np.mean(all_compound_means):.2f} ± {np.std(all_compound_means):.2f} ft\") \n",
    "print(f\"• Average rainfall contribution: {np.mean(all_rainfall_contribs):.2f} ± {np.std(all_rainfall_contribs):.2f} ft\")\n",
    "print(f\"• Rainfall contribution range: {np.min(all_rainfall_contribs):.2f} to {np.max(all_rainfall_contribs):.2f} ft\")\n",
    "\n",
    "# Print design storm set summary\n",
    "print(f\"\\nDESIGN STORM SET SUMMARY:\")\n",
    "for i, (percentile_range, storm_list) in enumerate(percentile_ranges.items()):\n",
    "    print(f\"• {set_names[i]}: {len(storm_list)} storms (Storm IDs: {sorted(storm_list)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03eaa70-ba5c-4f57-9e6c-be7e02ecf254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Set publication-quality style\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'DejaVu Serif'],\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "    'figure.dpi': 300,\n",
    "    'axes.linewidth': 1.0,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'axes.axisbelow': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.linewidth': 0.5,\n",
    "})\n",
    "\n",
    "# Calculate the 500-year storm WSE\n",
    "closest_row = storm_data_all_uncert_by_id.iloc[(storm_data_all_uncert_by_id['return_period'] - 500).abs().argsort()[:1]]\n",
    "storm_data_all_500_wse = closest_row['depth_raw'].values[0] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "\n",
    "# Get unique storm IDs\n",
    "unique_storm_ids = cdf_data_frames[idx][3]['storm_id'].unique()\n",
    "\n",
    "# Collect data for each storm\n",
    "storm_data_collection = []\n",
    "\n",
    "for storm_id in unique_storm_ids:\n",
    "    # Get compound flooding data (with rainfall)\n",
    "    compound_data = storm_data_all_uncert[\n",
    "        (storm_data_all_uncert['ras_id'] == closest_ras_ids[idx]) & \n",
    "        (storm_data_all_uncert['storm_id'] == storm_id)\n",
    "    ]\n",
    "    \n",
    "    # Get surge-only data (without rainfall)\n",
    "    surge_data = storm_data_surge_uncert[\n",
    "        (storm_data_surge_uncert['ras_id'] == closest_ras_ids[idx]) & \n",
    "        (storm_data_surge_uncert['storm_id'] == storm_id)\n",
    "    ]\n",
    "    \n",
    "    if len(compound_data) > 5 and len(surge_data) > 0:  # Require sufficient data\n",
    "        compound_wse = compound_data['depth_raw'] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "        surge_wse = surge_data['depth_raw'] + geo_frame_filtered_to_id['elevs'].values[0]\n",
    "        \n",
    "        storm_info = {\n",
    "            'storm_id': storm_id,\n",
    "            'compound_wse_values': compound_wse.values,\n",
    "            'compound_wse_min': np.min(compound_wse),\n",
    "            'compound_mean': np.mean(compound_wse),\n",
    "            'surge_mean': np.mean(surge_wse),\n",
    "            'surge_wse_values': surge_wse.values,\n",
    "            'hydrologic_contribution_percent': (np.mean(compound_wse) - np.mean(surge_wse)) / np.mean(compound_wse) * 100\n",
    "        }\n",
    "        storm_data_collection.append(storm_info)\n",
    "\n",
    "# Sort storms by their mean compound WSE\n",
    "storm_data_collection.sort(key=lambda x: x['compound_wse_min'])\n",
    "\n",
    "# ========================\n",
    "# Create design storm sets based on surge means\n",
    "# ========================\n",
    "percentiles = [0, 25, 50, 75, 100]\n",
    "percentile_ranges = {}\n",
    "\n",
    "# Extract storm_id and surge_mean\n",
    "surge_means = [(storm['storm_id'], storm['surge_mean']) for storm in storm_data_collection]\n",
    "surge_means.sort(key=lambda x: x[1])  # sort by surge_mean\n",
    "surge_values = [x[1] for x in surge_means]\n",
    "\n",
    "# Compute cutoff values\n",
    "cutoffs = np.percentile(surge_values, percentiles)\n",
    "\n",
    "# Assign storms to bins\n",
    "for i in range(len(percentiles)-1):\n",
    "    low, high = cutoffs[i], cutoffs[i+1]\n",
    "    storms_in_range = [sid for sid, val in surge_means if low <= val <= high]\n",
    "    percentile_ranges[f\"{percentiles[i]}-{percentiles[i+1]}\"] = storms_in_range\n",
    "\n",
    "# Create a mapping from storm_id to design set\n",
    "storm_to_set = {}\n",
    "set_names = ['Design Storm Set 1', 'Design Storm Set 2', 'Design Storm Set 3', 'Design Storm Set 4']\n",
    "\n",
    "for i, (percentile_range, storm_list) in enumerate(percentile_ranges.items()):\n",
    "    for storm_id in storm_list:\n",
    "        storm_to_set[storm_id] = i\n",
    "\n",
    "# Create the main figure with professional layout\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[4, 1], hspace=0.05)\n",
    "\n",
    "# Main plot\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "\n",
    "# Parameters\n",
    "bar_width = 0.65\n",
    "storm_positions = np.arange(len(storm_data_collection))\n",
    "\n",
    "# Professional color scheme\n",
    "surge_color = '#2E86AB'      # Deep blue\n",
    "compound_color = '#A23B72'    # Deep magenta/burgundy\n",
    "reference_color = '#F18F01'   # Orange\n",
    "set_colors = ['#1F4E79', '#8B4513', '#2F5233', '#8B0000']  # Professional darker tones\n",
    "\n",
    "# Plot each storm\n",
    "for i, storm_info in enumerate(storm_data_collection):\n",
    "    storm_id = storm_info['storm_id']\n",
    "    compound_wse_values = storm_info['compound_wse_values']\n",
    "    surge_mean = storm_info['surge_mean']\n",
    "    compound_mean = storm_info['compound_mean']\n",
    "    \n",
    "    x_pos = storm_positions[i]\n",
    "    \n",
    "    # 1. Plot surge-only bar (baseline contribution)\n",
    "    surge_bar = ax_main.bar(x_pos, surge_mean, width=bar_width, \n",
    "                           color=surge_color, alpha=0.8, edgecolor='white', linewidth=0.5,\n",
    "                           label='Storm Surge Component' if i == 0 else \"\")\n",
    "    \n",
    "    # 2. Enhanced violin plot for compound WSE variability\n",
    "    violin_data = compound_wse_values\n",
    "    \n",
    "    if len(violin_data) > 3:\n",
    "        # Calculate statistics\n",
    "        p25 = np.percentile(violin_data, 25)\n",
    "        p50 = np.percentile(violin_data, 50)  # median\n",
    "        p75 = np.percentile(violin_data, 75)\n",
    "        data_min = np.min(violin_data)\n",
    "        data_max = np.max(violin_data)\n",
    "        \n",
    "        # Create density estimation\n",
    "        kde = gaussian_kde(violin_data)\n",
    "        density_range = np.linspace(data_min, data_max, 150)\n",
    "        density_values = kde(density_range)\n",
    "        \n",
    "        # Normalize density for violin width\n",
    "        max_density = np.max(density_values)\n",
    "        density_norm = (density_values / max_density) * (bar_width / 3.5)\n",
    "        \n",
    "        # Plot violin (compound flooding variability)\n",
    "        ax_main.fill_betweenx(density_range, \n",
    "                             x_pos - density_norm, \n",
    "                             x_pos + density_norm,\n",
    "                             alpha=0.7, color=compound_color, \n",
    "                             label='Compound Flooding Variability' if i == 0 else \"\")\n",
    "        \n",
    "        # Add violin outline\n",
    "        ax_main.plot(x_pos - density_norm, density_range, color='darkred', linewidth=0.8, alpha=0.9)\n",
    "        ax_main.plot(x_pos + density_norm, density_range, color='darkred', linewidth=0.8, alpha=0.9)\n",
    "        \n",
    "        # Add statistical markers\n",
    "        marker_width = bar_width / 5\n",
    "        # Median line (white for visibility)\n",
    "        ax_main.plot([x_pos - marker_width, x_pos + marker_width], \n",
    "                    [p50, p50], 'white', linewidth=2.5, alpha=0.95)\n",
    "        \n",
    "        # IQR box (subtle)\n",
    "        iqr_width = bar_width / 8\n",
    "        ax_main.plot([x_pos - iqr_width, x_pos + iqr_width], \n",
    "                    [p25, p25], 'white', linewidth=1.5, alpha=0.8)\n",
    "        ax_main.plot([x_pos - iqr_width, x_pos + iqr_width], \n",
    "                    [p75, p75], 'white', linewidth=1.5, alpha=0.8)\n",
    "        \n",
    "        # Whiskers (connecting to extremes)\n",
    "        ax_main.plot([x_pos, x_pos], [data_min, p25], color='darkred', \n",
    "                    linewidth=1.2, alpha=0.7, linestyle='-')\n",
    "        ax_main.plot([x_pos, x_pos], [p75, data_max], color='darkred', \n",
    "                    linewidth=1.2, alpha=0.7, linestyle='-')\n",
    "\n",
    "# Reference line for 500-year WSE\n",
    "ax_main.axhline(y=storm_data_all_500_wse, color=reference_color, linestyle='--', \n",
    "                linewidth=2.5, label=f'500-year Return Period WSE ({storm_data_all_500_wse:.1f} ft)', \n",
    "                alpha=0.9, zorder=10)\n",
    "\n",
    "# Customize main plot\n",
    "ax_main.set_ylabel('Water Surface Elevation (ft NAVD88)', fontweight='bold')\n",
    "ax_main.set_title('Storm-by-Storm Water Surface Elevation Analysis\\nAmite River at 45 km Upstream', \n",
    "                 fontweight='bold', pad=15)\n",
    "\n",
    "# Set x-axis (will be handled by subplot below)\n",
    "ax_main.set_xticks(storm_positions)\n",
    "ax_main.set_xticklabels([])  # Remove x-labels from main plot\n",
    "\n",
    "# Professional legend\n",
    "legend = ax_main.legend(loc='upper left', frameon=True, fancybox=False, \n",
    "                       shadow=False, edgecolor='black', facecolor='white', \n",
    "                       framealpha=0.95, borderpad=0.8)\n",
    "legend.get_frame().set_linewidth(0.8)\n",
    "\n",
    "# Enhanced grid\n",
    "ax_main.grid(True, alpha=0.4, linestyle='-', linewidth=0.5)\n",
    "ax_main.set_axisbelow(True)\n",
    "\n",
    "# ========================\n",
    "# Design Storm Set Classification (bottom panel) - FIXED\n",
    "# ========================\n",
    "ax_sets = fig.add_subplot(gs[1])\n",
    "\n",
    "# Get positions for each design storm set\n",
    "set_positions = {}\n",
    "for i, storm_info in enumerate(storm_data_collection):\n",
    "    storm_id = storm_info['storm_id']\n",
    "    set_index = storm_to_set.get(storm_id, 0)\n",
    "    if set_index not in set_positions:\n",
    "        set_positions[set_index] = []\n",
    "    set_positions[set_index].append(storm_positions[i])\n",
    "\n",
    "# Draw design storm set classifications with proper positioning\n",
    "for set_index, positions in set_positions.items():\n",
    "    if len(positions) > 0:\n",
    "        color = set_colors[set_index % len(set_colors)]\n",
    "        \n",
    "        # Calculate proper rectangle bounds\n",
    "        left_pos = min(positions) - bar_width/2\n",
    "        right_pos = max(positions) + bar_width/2\n",
    "        rect_width = right_pos - left_pos\n",
    "        ##\n",
    "\n",
    "        rect_height = 0.08  # your choice\n",
    "        rect_y = -rect_height/2  #\n",
    "        # Background rectangle - properly sized around the text area\n",
    "        rect = Rectangle((left_pos, rect_y), rect_width, rect_height, \n",
    "                        facecolor=color, alpha=0.15, edgecolor=color, \n",
    "                        linewidth=1.5)\n",
    "        ax_sets.add_patch(rect)\n",
    "        \n",
    "        # Set label - centered properly\n",
    "        center_pos = np.mean(positions)\n",
    "        ax_sets.text(center_pos, 0, set_names[set_index], \n",
    "                    ha='center', va='center', fontweight='bold',\n",
    "                    fontsize=10, color=color)\n",
    "\n",
    "# Configure bottom panel\n",
    "ax_sets.set_xlim(ax_main.get_xlim())\n",
    "ax_sets.set_ylim(-0.1,0.1)\n",
    "ax_sets.set_xlabel('Storm ID (JPM Storm Suite)', fontweight='bold')\n",
    "\n",
    "# Storm ID labels\n",
    "storm_labels = [f\"{storm['storm_id']}\" for storm in storm_data_collection]\n",
    "ax_sets.set_xticks(storm_positions)\n",
    "ax_sets.set_xticklabels(storm_labels, rotation=45, ha='right')\n",
    "ax_sets.set_yticks([])\n",
    "\n",
    "# Remove spines for clean look\n",
    "for spine in ax_sets.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# REMOVED the separator line that was causing the grey line issue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.12)  # Extra space for rotated labels\n",
    "\n",
    "# ========================\n",
    "# Professional Summary Statistics\n",
    "# ========================\n",
    "print(\"=\" * 100)\n",
    "print(\"STORM-BY-STORM WATER SURFACE ELEVATION DECOMPOSITION ANALYSIS\")\n",
    "print(\"Location: Amite River, 45 km upstream\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Reference 500-Year WSE: {storm_data_all_500_wse:.2f} ft NAVD88\")\n",
    "print()\n",
    "\n",
    "# Enhanced table format\n",
    "print(f\"{'Storm ID':<8} | {'Design Set':<12} | {'Surge WSE':<10} | {'Compound WSE':<13} | {'Rainfall Δ':<11} | {'Variability Range':<16} | {'Exceedance':<12}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "total_realizations = 0\n",
    "total_exceeds_500 = 0\n",
    "\n",
    "for storm_info in storm_data_collection:\n",
    "    storm_id = storm_info['storm_id']\n",
    "    surge_mean = storm_info['surge_mean']\n",
    "    compound_mean = storm_info['compound_mean']\n",
    "    compound_values = storm_info['compound_wse_values']\n",
    "    set_index = storm_to_set.get(storm_id, 0)\n",
    "    \n",
    "    rainfall_contrib = compound_mean - surge_mean\n",
    "    variability_range = f\"{np.min(compound_values):.1f}–{np.max(compound_values):.1f} ft\"\n",
    "    \n",
    "    exceeds_500 = np.sum(compound_values >= storm_data_all_500_wse)\n",
    "    total_exceeds_500 += exceeds_500\n",
    "    total_realizations += len(compound_values)\n",
    "    \n",
    "    exceedance_pct = exceeds_500/len(compound_values)*100\n",
    "    \n",
    "    print(f\"{storm_id:<8} | {'Set ' + str(set_index+1):<12} | {surge_mean:>8.2f} ft | {compound_mean:>11.2f} ft | {rainfall_contrib:>9.2f} ft | {variability_range:<16} | {exceeds_500:>3}/{len(compound_values):<3} ({exceedance_pct:4.1f}%)\")\n",
    "\n",
    "print(\"-\" * 105)\n",
    "print(f\"TOTAL ANALYSIS: {total_exceeds_500:,}/{total_realizations:,} realizations exceed 500-year WSE ({total_exceeds_500/total_realizations*100:.1f}%)\")\n",
    "\n",
    "# Enhanced insights\n",
    "print(f\"\\nKEY FINDINGS:\")\n",
    "max_surge_storm = max(storm_data_collection, key=lambda x: x['surge_mean'])\n",
    "max_compound_storm = max(storm_data_collection, key=lambda x: x['compound_mean'])\n",
    "max_rainfall_contrib_storm = max(storm_data_collection, key=lambda x: x['compound_mean'] - x['surge_mean'])\n",
    "\n",
    "print(f\"• Maximum storm surge component: Storm {max_surge_storm['storm_id']} ({max_surge_storm['surge_mean']:.2f} ft)\")\n",
    "print(f\"• Maximum compound WSE: Storm {max_compound_storm['storm_id']} ({max_compound_storm['compound_mean']:.2f} ft)\")\n",
    "print(f\"• Maximum rainfall contribution: Storm {max_rainfall_contrib_storm['storm_id']} (+{max_rainfall_contrib_storm['compound_mean'] - max_rainfall_contrib_storm['surge_mean']:.2f} ft)\")\n",
    "\n",
    "# Statistical summary\n",
    "all_surge_means = [storm['surge_mean'] for storm in storm_data_collection]\n",
    "all_compound_means = [storm['compound_mean'] for storm in storm_data_collection]\n",
    "all_rainfall_contribs = [storm['compound_mean'] - storm['surge_mean'] for storm in storm_data_collection]\n",
    "\n",
    "print(f\"\\nSTATISTICAL SUMMARY (n={len(storm_data_collection)} storms):\")\n",
    "print(f\"• Storm surge component: {np.mean(all_surge_means):.2f} ± {np.std(all_surge_means):.2f} ft\")\n",
    "print(f\"• Compound WSE: {np.mean(all_compound_means):.2f} ± {np.std(all_compound_means):.2f} ft\") \n",
    "print(f\"• Rainfall contribution: {np.mean(all_rainfall_contribs):.2f} ± {np.std(all_rainfall_contribs):.2f} ft\")\n",
    "print(f\"• Rainfall contribution range: {np.min(all_rainfall_contribs):.2f} to {np.max(all_rainfall_contribs):.2f} ft\")\n",
    "\n",
    "print(f\"\\nDESIGN STORM CLASSIFICATION:\")\n",
    "for i, (percentile_range, storm_list) in enumerate(percentile_ranges.items()):\n",
    "    surge_range = [storm['surge_mean'] for storm in storm_data_collection if storm['storm_id'] in storm_list]\n",
    "    print(f\"• {set_names[i]}: {len(storm_list)} storms | Surge range: {min(surge_range):.1f}–{max(surge_range):.1f} ft\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8122191-db8f-4a48-b138-ebe3c99f4730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pareto\n",
    "\n",
    "closest_row = storm_data_all_uncert_by_id.iloc[(storm_data_all_uncert_by_id['return_period'] - 500).abs().argsort()[:1]]\n",
    "storm_data_all_500_wse = closest_row['depth_raw'].values[0]+geo_frame_filtered_to_id['elevs'].values[0]\n",
    "storm_data_all_500_wse \n",
    "\n",
    "# Select data for storm id 498 and ras_id 77456\n",
    "storm_data_all_uncert_by_all_id = storm_data_all_uncert[(storm_data_all_uncert['ras_id'] == \n",
    "closest_ras_ids[idx]) & (storm_data_all_uncert['storm_id'] == 33) ]\n",
    "\n",
    "wse_data = storm_data_all_uncert_by_all_id['depth_raw']+geo_frame_filtered_to_id['elevs'].values[0]\n",
    "\n",
    "# Fit a Pareto distribution to the data\n",
    "shape, loc, scale = pareto.fit(wse_data)\n",
    "\n",
    "# Plot the PDF of depth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(wse_data, bins=15, color='blue', edgecolor='black', density=True, alpha=0.6, label='Data')\n",
    "\n",
    "# Plot the fitted Pareto distribution\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = pareto.pdf(x, shape, loc, scale)\n",
    "plt.plot(x, p, 'k', linewidth=2, label=f'Fit: shape={shape:.2f}, loc={loc:.2f}, scale={scale:.2f}')\n",
    "\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('PDF of Depth for Storm ID 498 and RAS ID')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5a5e29-3b65-467c-8484-1f724fa257b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pareto\n",
    "\n",
    "storm_id = 80\n",
    "\n",
    "# Select data for storm id 498 and ras_id 77456\n",
    "storm_data_all_uncert_by_all_id = storm_data_all_uncert[(storm_data_all_uncert['ras_id'] == \n",
    "closest_ras_ids[idx]) & (storm_data_all_uncert['storm_id'] == storm_id) ]\n",
    "\n",
    "wse_data = storm_data_all_uncert_by_all_id['depth_raw']+geo_frame_filtered_to_id['elevs'].values[0]\n",
    "\n",
    "# Fit a Pareto distribution to the data\n",
    "shape, loc, scale = pareto.fit(wse_data)\n",
    "\n",
    "# Plot the PDF of depth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(wse_data, bins=15, color='blue', edgecolor='black', density=True, alpha=0.6, label='Data')\n",
    "\n",
    "# Plot the fitted Pareto distribution\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = pareto.pdf(x, shape, loc, scale)\n",
    "plt.plot(x, p, 'k', linewidth=2, label=f'Fit: shape={shape:.2f}, loc={loc:.2f}, scale={scale:.2f}')\n",
    "\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('PDF of Depth for Storm ID 498 and RAS ID')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc56f4d6-7a65-48d7-8173-fad7caa23e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Select unique storm IDs from storm_data_surge_uncert\n",
    "unique_storm_ids = cdf_data_frames[idx][3]['storm_id'].unique()\n",
    "\n",
    "# Filter storm_data_surge_uncert for unique storm IDs using isin\n",
    "filtered_storm_data = storm_data_surge_uncert[(storm_data_surge_uncert['ras_id'] == \n",
    "closest_ras_ids[idx]) & (storm_data_surge_uncert['storm_id'].isin(unique_storm_ids))]\n",
    "\n",
    "# Check if we have data\n",
    "if len(filtered_storm_data) == 0:\n",
    "    print(\"No data found for the specified ras_id and storm_ids\")\n",
    "else:\n",
    "    print(f\"Found {len(filtered_storm_data)} records for {len(unique_storm_ids)} storms\")\n",
    "    \n",
    "    # Get depth_raw values and remove any NaN values\n",
    "    depth_raw_values = filtered_storm_data['depth_raw'].dropna().values+geo_frame_filtered_to_id['elevs'].values[0]\n",
    "    \n",
    "    if len(depth_raw_values) == 0:\n",
    "        print(\"No valid depth_raw values found\")\n",
    "    else:\n",
    "        # Calculate the PDF using Gaussian Kernel Density Estimation\n",
    "        kde = gaussian_kde(depth_raw_values)\n",
    "        x_range = np.linspace(depth_raw_values.min(), depth_raw_values.max(), 1000)\n",
    "        pdf_values = kde(x_range)\n",
    "        \n",
    "        # Plot the PDF\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_range, pdf_values, label=f'All Storms (n={len(unique_storm_ids)})', linewidth=2)\n",
    "        plt.xlabel('WSE (ft)', fontsize=12)\n",
    "        plt.ylabel('Probability Density', fontsize=12)\n",
    "        plt.title(f'PDF of Depth Raw for {len(unique_storm_ids)} Storms at RAS ID {closest_ras_ids[idx]}', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add summary statistics as text\n",
    "        plt.text(0.02, 0.98, f'Mean: {np.mean(depth_raw_values):.2f} ft\\n'\n",
    "                            f'Std: {np.std(depth_raw_values):.2f} ft\\n'\n",
    "                            f'Samples: {len(depth_raw_values)}', \n",
    "                transform=plt.gca().transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Storm IDs included: {unique_storm_ids}\")\n",
    "        print(f\"WSE range: {depth_raw_values.min():.3f} - {depth_raw_values.max():.3f} ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f90130-b98a-44b1-a6b8-0a3d7b65f5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Filter data for the closest_ras_ids[0]\n",
    "data_to_plot = storm_data_surge_uncert[storm_data_surge_uncert['ras_id'] == closest_ras_ids[0]]['depth_adj']\n",
    "\n",
    "# Calculate the PDF using Gaussian Kernel Density Estimation\n",
    "kde = gaussian_kde(data_to_plot)\n",
    "x_range = np.linspace(data_to_plot.min(), data_to_plot.max(), 1000)\n",
    "pdf_values = kde(x_range)\n",
    "\n",
    "# Plot the PDF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_range, pdf_values, color='blue', label='PDF of depth_adj')\n",
    "plt.xlabel('Depth Adjusted')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('PDF of Depth Adjusted for RAS ID {}'.format(closest_ras_ids[0]))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de9e9abd-9410-4ff9-a832-4cbb9334b0f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Figures10,11,12",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
